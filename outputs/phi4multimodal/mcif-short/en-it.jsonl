{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuto alla nostra presentazione di DeepL, un nuovo corpus per la identificazione del testo in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo da parte di un gruppo di destinazione specifico, come persone con problemi di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, potete vedere una coppia di frasi parallele di una frase complessa tedesca e la sua traduzione in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, la dilatazione della clausola, la reordinazione della dilatazione della clausola o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ora proponiamo il nostro nuovo corpus, perché negli ultimi anni, ci sono stati alcuni problemi con i corpora esistenti. Quindi, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che sono proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori in loro allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo corpus dlan, che è diviso in due sottocorpora, dlan apa e dlan web. Dlan apa è basato su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In Deep-Lane APA, abbiamo allineato quattro ottantatre documenti, tutti manualmente. Questo ha portato a circa tredicimila coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per deepnlpweb, questo corpus include diversi domini. E abbiamo anche allineato tutti questi settecentocinquanta documenti a mano, e dall'altra parte con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto trentamilaquattrocentocinquanta coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più i nostri coppie di frasi. Ad esempio, sul tipo di semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come puoi vedere qui, i testi della Bibbia sono molto più forti, semplificati rispetto, ad esempio, ai testi delle notizie o dei testi di apprendimento della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Su tutti i livelli, per esempio, semplificazione lessicale, semplificazione strutturale, o il livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, puoi vedere che il nostro corpus di dipiano ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus di API di dipiano, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto al corpus di web di dipiano."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel Web Corpus, abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro set di dati deep plane. Quindi per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "In recent years, there have been a lot of alignment methods, but in the context of machine translations."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, che hanno lo stesso contenuto, ma sono a un livello di complessità diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora, poiché abbiamo il nostro set di dati, D plane, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo fatto alcune adattazioni ai metodi proposti. E abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior allineamento, il metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco, è il metodo di mass align."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti in carta."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso di utilizzo che abbiamo mostrato nel nostro articolo è il caso della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "Per affinare i modelli linguistici per produrre testo semplificato dal testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affinato due modelli diversi. Abbiamo affinato il modello di lungimiranza per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche affinato l'imparto normale di base per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Puoi anche trovare tutti i checkpoint e puoi esaminare in modo più dettagliato i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questa, questa, la fine tuning di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base, per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skorkowsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come potete sapere, ci sono diverse strutture di dipendenza presunte da diverse teorie e approcci di corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "È tale che il primo coniugato è la testa dell'intera struttura di coordinate. Quindi, in questo caso, Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è presupposto nella teoria del testo di significato di Egor Miltjuk, dove, ancora una volta, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, giusto? Sceglieranno uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio prague, l'approccio congiuntivo, che si suppone in banche a albero di dipendenza prague, dove le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo alcune dipendenze da e a tutti i contratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E infine, c'è anche un approccio multiheaded che viene utilizzato, ad esempio, nella grammatica delle parole di De Caton."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "Dove, per così dire, tutti i congiunti sono testa della struttura codificata. Quindi otteniamo dipendenze dal governatore qui lovs a tutti i congiunti separatamente. Questi sono i bot e me."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, lo scopo di questo articolo è quello di produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due, e contro le strutture asimmetriche di coordinazione come queste."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "O in inglese, come si sa, i soggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani, giusto? Quindi, marzo, letto ieri va bene perché il soggetto diretto, esso, è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre \"mardi\" legge \"ieri\", è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggettivo \"ieri\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché in tal caso può essere spostato nella posizione successiva all'aggancio."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. March ha letto questo libro assolutamente affascinante sulla Bcysd. Va bene, dove invece di it, abbiamo questo lungo e piatto."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va anche detto che Martorellos ha scritto ieri questo libro assolutamente affascinante sui formiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi la ragione qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più corte sono preferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza da red a l'addio di lunghezza sette misurata in parole, e da red a book di lunghezza quattro, quindi per ottenere undici."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, invece di undici, sei, molto più breve. Ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata della banca a nastro e della carta, perché non useremmo dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione fatta molte volte prima, che i congiuntivi di sinistra tendono ad essere più brevi. Quindi sale e pepe e non pepe e sale misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che è stata fatta in passato, che questa tendenza cresce con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è più grande del congiunto più breve a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando le governanze a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Giusto? Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi è il governatore, è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio. Homer è venuto a starnutire. Qui abbiamo la coordinazione di due verbi e non c'è un governante esterno, giusto? Quindi, in tali casi, il congiuntivo sinistro preferisce essere più breve, più grande, la differenza tra i due."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è a destra, come qui, a sinistra governa la coordinazione del network, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Quindi lo dimostriamo misurando la lunghezza in caratteri, la prima colonna, in sillabe, la colonna centrale e in parole, la colonna destra. Quindi mi concentrerò sulla destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governo è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per il congiunto di sinistra per essere più breve cresce costantemente con la differenza assoluta delle parole. E lo stesso si osserva quando non c'è governante, come nella coordinazione delle frasi. Ma quando il governante è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E mostriamo nel documento come questo fornisce un argomento contro le strutture asimmetriche di coordinazione come queste due e per le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, leggi il documento per l'accordo completo e argomenti, scusa, e parlaci della sessione di follow-up. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbing, PhD student all'Università di Washington. Oggi presento il nostro lavoro, dal pretraining dei dati ai modelli linguistici, alle attività downstream, monitorando i percorsi dei pregiudizi politici che portano a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i modelli linguistici vengono addestrati su grandi campioni di dati web."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di notizie politiche sono ben coperti nei loro dati di pretraining. Secondo un sondaggio sul corpus C4, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di allenamento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato un benedetto misto per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebra la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudizievoli e potrebbero portare a potenziali problemi di equità nelle applicazioni di task downstream."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "Per questo scopo, proponiamo di indagare sulla pipeline di propagazione del pre bias politico, dai dati di pre training ai modelli linguistici ai compiti downstream, in particolare ponendo le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere il pretraining sui bias politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come funzionano effettivamente i modelli linguistici con diverse linee politiche su compiti downstream e se ciò potrebbe comportare problemi di equità nell'uso di applicazioni NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in particolare, abbiamo prima proposto di stimare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test di compassione politica. Questo ci consente di fare valutazioni automatiche ben radicate nella letteratura di scienza politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Quindi alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno una variegata inclinazione politica. Occupano tutti e quattro i quadranti del campo politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche vedere che GPT four è il modello di linguaggio più liberale di tutti, e le teorie di GPT sono generalmente più socialmente liberali delle teorie di Burt e delle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, abbiamo lo scopo di indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente presi in considerazione dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto un esperimento controllato pre-allenando i checkpoint del modello linguistico su sei diversi corpus di partiti separati in notizie e social media ulteriormente divisi in loro inclinazione politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Perché, inoltre, pre-addestrare i modelli linguistici su tali corpora parziali, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano corrispondentemente."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta ulteriormente affinata ulteriormente addestrata sul corpus di Reddit di tendenza sinistra, possiamo vedere un sostanziale spostamento liberale in termini di."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "In termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche cercato di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora pre-allenamento in pre-Quarantacinquesimo presidente degli Stati Uniti e dopo Quarantacinquesimo presidente degli Stati Uniti. Pre-trainamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "We can see that language models generally had a political leaning that is further away from the center after twenty seventeen so this indicates that language models can also pick up the like polarization in our society."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per non parlare del resto, valutiamo i modelli linguistici con diversi significati politici sulla rilevazione del discorso di odio e sulla rilevazione di notizie false, due applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che se indagiamo sulle prestazioni per categoria, vale a dire, se separiamo le prestazioni in."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "A rilevare il suo discorso di odio, rivolto a gruppi socialmente minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono peggio a rilevare l'odio, il discorso, l'obiettivo di più gruppi di potere nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli di linguaggio scritti sono migliori nel rilevare discorsi d'odio rivolti ai bianchi e agli uomini, ma peggiori nel rilevare discorsi d'odio rivolti a comunità nere, LGBTI e altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Trends simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di slittamento laterale sono migliori nel rilevare informazioni errate dal loro orientamento politico opposto e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo, mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello di linguaggio lineare destra dovesse essere affinato su discorsi di odio o disinformazione o qualsiasi altra cosa, e implementato su una piattaforma di social media popolare,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e il discorso di odio che prende di mira i gruppi di minoranza potrebbe semplicemente dilaniarli senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivati dai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi un po' di discussione. Vorremmo anche sottolineare che abbiamo esposto il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come tra Ceylon e Karibdis."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanitizziamo le opinioni politiche nei dati di allenamento del modello linguistico, il pregiudizio si propagherà dai dati di pre-allenamento ai modelli linguistici ai compiti downstream, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se proviamo a sanificarlo in qualche modo, rischiamo anche censura o esclusione. Ed è incredibilmente difficile determinare cosa è effettivamente neutro e dovrebbe essere mantenuto. Quindi è un po' come il problema del carrello elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Ok, grande. Penso che sia più o meno tutto ciò che ho per oggi. Grazie per il tuo tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, uno studente di primo anno di dottorato presso Carnegie Mellon University e oggi presenterò il vostro lavoro anal positionality, che caratterizza i pregiudizi di progettazione dei set di dati e dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcune persone dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Rohan Labrosse, Katerina Raneeka e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Quindi iniziamo immaginando di lavorare per un giornale e di sfogliare i commenti sotto al tuo articolo di notizie, cercando di rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti orientarti a un popolare api come l'api di prospettiva per la rilevazione della tossicità. E questo funziona davvero bene se sei Carl Jones, dove l'api di prospettiva è in grado di rilevare correttamente le istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio così per Dithya Sharma, dove l'API prospettiva non è davvero sensibile a termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione, in cui vediamo differenze sistematiche delle prestazioni della tecnologia tra popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Design bias, come quello che abbiamo appena visto prima, potrebbe verificarsi a causa della positionalità dei ricercatori e degli sviluppatori di modelli. La positionalità è semplicemente la prospettiva che le persone hanno come risultato della loro demografia, identità e esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare nello spazio accademico femminista e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, la positionalità può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che la gente potrebbe porre è: i set di dati e i modelli hanno posizione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di dire che i modelli e i set di dati stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionamento, come lacune culturali e modelli e set di dati, così come definizioni teoriche di posizionamento del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori in realtà non esaminano il confronto tra utenti finali e set di dati e modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E studiare la posizione del modello e del set di dati è sempre più importante man mano che le attività NLP diventano più soggettive e socialmente orientate."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare come queste posizioni sono distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per studiare la posizionabilità del set di dati e del modello, confrontiamo le annotazioni con utenti reali con i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro quadro di posizione, nlpositionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due passaggi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è reannotare i set di dati con vari annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E preferiamo farlo guardando alla demografia degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie sono raramente raccolte e condivise."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E così scegliamo di reannotare i dati per ottenere molte annotazioni, per esempio, e per ottenere un set ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione Pearson."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E quindi il nostro quadro in realtà differisce dalla letteratura di disaccordo degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni e etichette, invece di guardare solo all'accordo degli annotatori o modellare le distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte abilitato attraverso Lab in the Wild, una piattaforma di crowdsourcing online, ex collaboratore HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "E Lab in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi rispetto alle piattaforme come M turk, che in gran parte hanno partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Organizziamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale e poi scriveranno quanto sia accettabile socialmente una situazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Dopo, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e con quelle degli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la chimica sociale, delphi e gpt four."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo replicato un setup molto simile per il compito di rilevamento della tossicità e della discostanza. In cui leggeranno un'istanza da Dinhate e scriveranno se pensano che sia un'istanza di discostanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con dynahate perspective api, rewire api, hate roberta e gpt four. Il nostro studio, alla fine, ha raccolto oltre sedicimila annotazioni da oltre mille annotatori provenienti da ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora siamo meglio attrezzati per rispondere a chi si allinea di più con i set di dati e i modelli NLP. Scopriamo che c'è una posizione nella Nlp."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, scopriamo che i set di dati e i modelli sono più in linea con i paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale del GPD4, scopriamo che è più in linea con i paesi di Confucio e di lingua inglese. Scopriamo che anche Dinahite è più in linea con i paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche la maggior parte di ul aggiuntiva allineamento con le persone che hanno un'istruzione universitaria. Quindi per gpt four nel compito di accettabilità sociale, troviamo che è più allineato con le persone con un'istruzione universitaria o un'istruzione di laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo lo stesso per Donnie Hatt, dove è più in linea con le persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di questo è che i set di dati e i modelli sono meno simili alle persone non binarie rispetto ai loro omologhi maschili e femminili. Lo troviamo nel compito di accettabilità sociale di gpd four, così come nell'analisi del compito di odio di Dine."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che c'è una posizione in allidina e l p, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di design rilevanti durante il processo di ricerca. E l'altra è fare ricerche di NLP con una lente di perspectivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire set di dati specializzati e modelli all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa Muscokani. Voglio dire, vogliamo sottolineare che l'inclusività nel campo della tecnologia non si limita a rendere, sai, tutte le tecnologie funzionali per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E così conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di dare un'occhiata al nostro dashboard per i risultati di analisi più aggiornati e al nostro documento. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Si Yu Yuan della Fudan University. Sono qui per presentare il nostro lavoro, distinguere la conoscenza del testo dai modelli linguistici di grandi dimensioni per la pianificazione del linguaggio vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e ha dimostrato che i grandi modelli linguistici possono decomporsi efficacemente gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il lavoro precedente si concentra principalmente sulla pianificazione degli obiettivi astratti delle attività stereotipate, pianificando gli obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiato."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, definiamo il problema della pianificazione del linguaggio vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impone diversi vincoli ai piani di obiettivo. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multiformi. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, valutiamo e miglioriamo la capacità di pianificazione del linguaggio vincolata di modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Poiché non esiste un set di dati specifico per supportare il nostro studio."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo acquisire questi obiettivi prima. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione dei dati umani in un ciclo, usando l'istruzione. Gpt."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Proviamo centinaia di giochi specifici e valutiamo gli script generati dai modelli di linguaggio avanzati."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli di linguaggio naturale ottengono risultati insoddisfacenti per la pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi conduciamo analisi dettagliate per indagare perché i modelli di livello medio falliscono."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati della figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scavato in una categoria di argomenti più ampia, le categorie di vincoli definiti nel wiki. La mappa di calore nella figura mostra che le prestazioni di pianificazione dei gestori di instruitori variano considerevolmente per ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di linguaggio cade in alta varianza, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di filtro overgenerated then per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo prima i tipi di vincoli con esempi per instradare gpt e ottenere obiettivi specifici in base agli obiettivi astratti di said."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi istruisci GPT a generare script chiave per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare i script facili."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e goal in embedding di istruzioni GPT e calcoliamo la somiglianza di coseno e i punteggi di somiglianza per misurare la somiglianza semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, escludiamo lo script che contiene le parole chiave del vincolo di destinazione. Conserveremo lo script solo se l'obiettivo raggiunge il punteggio più alto nel set di obiettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, instragram può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità, sia nella completezza semantica che nella fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di set di dati è un passo essenziale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione di set di dati manuali è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, seguiremo l'idea di distillazione del sapere simbolico per distillare i set di dati di pianificazione del linguaggio vincolati dai grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata denominato coscript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo cinquantacinquemila obiettivi specifici con script. Per garantire la qualità della convalida e dei set di test, chiediamo ai lavoratori di Crowdsourced di trovare e rivedere i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione vincolata del codice script. Scopriamo che il codice script mostra un'alta plausibilità nelle specifiche generate. Con il codice script, possiamo creare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che il fine-tuning di T five su un set di codice può generare script di qualità superiore rispetto alla maggior parte dei modelli di linguaggio di grandi dimensioni, indicando che i modelli più piccoli possono supportare i modelli più grandi quando sono correttamente addestrati su set di dati adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio vincolato. Abbiamo valutato la capacità di pianificazione del linguaggio vincolato di modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro di generazione eccessiva per modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, coscript, per la pianificazione del linguaggio vincolata. Speriamo che il set di dati coscript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione del linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il tuo tempo. Per ulteriori dettagli su Coldscript, consulta il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo, \"Do Concol 2003 Named Entity Taggers Still Work Well in 2023\". Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro documento ha indagato sul problema della generalizzazione usando il compito di riconoscimento delle entità nominate, o il compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che i modelli hanno usato Conll two thousand three per sviluppare ner per quasi vent'anni. E questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il set di dati Conll. Questo è un set di dati che abbiamo raccolto da Reuters News da venti venti, e poi annotato con le stesse linee guida di annotazione di Conll duemilatre."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi affinato oltre venti modelli su colon 2003. Li abbiamo valutati sia sul set di prova colon 03 che sul set di prova colon plus plus."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, per ultimo, ma non meno importante, abbiamo calcolato la variazione percentuale in F uno per valutare la generalizzazione di ogni modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli di trasformatori normalmente generalizzano meglio su nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito, i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E, per ultimo, ma non meno importante, sappiamo tutti che il numero di esempi di fine tuning influisce direttamente sulle prestazioni di un compito downstream. Qui abbiamo anche scoperto che più esempi di fine tuning in realtà portano anche a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo ripetuto dello stesso set di test. E questo è di solito manifestato come il ritorno della diminuzione su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che, dal grafico a destra, la linea di miglior adattamento rossa ha un gradiente maggiore di uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo realizzato su kernel duemilatre si traduce in più di un'unità di miglioramento su kernel, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci mostra che l'overfitting adattivo, in questo caso, non è osservato."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "So what about temporary truffles?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti. E abbiamo scoperto che le prestazioni peggiorano con maggiore distanza temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi secondo cui la causa principale del calo delle prestazioni è il drift temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, dimensioni del modello più grandi, nonché più esempi di ottimizzazione fine. E questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da drifte temporali e, in qualche modo sorprendentemente, non è causato dall'overfitting adattivo, anche se Conll 2003 è stato utilizzato per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo sollevato all'inizio del nostro articolo, i tagger di Connell del 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un rotto, sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo sollevi la necessità di ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "E infine, assicurati di controllare il nostro documento, il nostro set di dati. E se hai domande, sentiti libero di contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Ciao. E parlerò del nostro lavoro sulla risoluzione di espressioni di corrispondenza diretta per la selezione di entità, in cui introduciamo il corpus di altentity."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "E il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Filip Radlinski, Silvia Parati e Anil Krish."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è capire la lingua degli utenti quando vogliono fare una scelta. E considera questa domanda alternativa, intendevi facile per me o ho un'idea qui? Un utente vuole selezionare tra uno di questi due siti."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è usare un riferimento diretto, per esempio, dicendo il nome della canzone è Easy on Me o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un indice di amico è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Allora le pronunce sono troppo simili l'una all'altra e difficili da disambiguare."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi in riferimenti diretti, ad esempio, il più recente o il più giovane che non è energetico."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dell'understanding dell'entità di LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non abbiamo un set di dati pubblico, un set di dati pubblico su larga scala per un compito, quindi ne raccogliamo uno usando l'annotazione della folla. Il nostro set di dati copre tre domini diversi: musica, libri e."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei set di dati enfatizza l'informality usando un set di completamento di cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartone ha tre bolle di discorso. Nella prima bolla, Bob dice, ricorda quella canzone che stavamo ascoltando ieri? E con questo, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo discorso, Alice dice: \"Vuoi dire, facendomi sentire a mio agio, o ho capito male?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "Qual è la domanda alternativa? E nella terza bolla del discorso, Bob usa un riferimento indiretto per selezionare una di queste entità. Ad esempio, il nuovo."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda bolla vocale, ma la terza viene compilata dall'annotatore. La prima bolla vocale viene scelta da alcuni prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, è generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo sempre un modello semplice. Vuoi dire A o B, dove A e B sono campioni di Wikipedia?"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro, e di solito è più difficile fare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'attacco uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome The Return."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno caselle di informazioni simili o attributi su Wikipedia, ad esempio, lo stesso genere o lo stesso artista per esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente conoscono l'entità."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quindi quello che facciamo è mostrare alcune conoscenze di fondo sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "E poi chiedi agli annotatori di ascoltare almeno alcune di ogni canzone e leggere su ogni canzone. Ecco, per esempio, il risultato della ricerca su Google per la canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio di ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo ulteriormente le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e di descriverle usando tre o cinque espressioni indirettamente riferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi del nostro set di dati. Ad esempio, quello senza parole, non quello con il bambino di dodici anni, o quello immaginario, o viene dall'Azerbaigian e"}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus di entità ha seimila domande alternative in tre domini, e ha quarantaduemila espressioni indirettamente riferite. I risultati con il modello T five x sono riassunti qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso background knowledge degli annotatori, allora l'accuratezza è davvero alta. È intorno al novantadue al novantacinque percento. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a alcune conoscenze di fondo parzialmente sovrapposte, allora l'accuratezza è compresa tra l'ottantadue e l'ottantasette percento, che è più realistica. Ad esempio, quando il modello linguistico recupera le conoscenze di fondo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del sessanta per cento. Quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili. Ecco un collegamento al nostro set di dati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Papi, dell'Università di Trento e Fondazione Bruno Kessler, e introdurrò brevemente il documento \"Attenzione come guida per la traduzione simultanea del parlato\", che è un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cosa è la traduzione vocale simultanea? La traduzione vocale simultanea, o SimulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli di simulazione attuali? Le architetture specifiche sono di solito addestrate introducendo ulteriori moduli da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complicate, ad esempio, la formazione che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, per utilizzare già esistenti modelli sd offline senza riaddestramento o adottare una specifica architettura per sd simile, utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfrutta le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, cioè il meccanismo di attenzione incrociata. E potete vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un dat o encoder decoder attention, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Un'unità viene emessa se la tensione non è concentrata, cioè questa somma è inferiore a un certo soglia alfa, verso gli ultimi frame di discorso lambdico, il che significa che le informazioni ricevute non sono abbastanza stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se riceviamo un frammento di discorso contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco,"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E vedremo i pesi di attenzione incrociati."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi frame di discorso ricevuti, mentre l'ultima parola indica i frame di discorso più recenti, come i frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Questo significa che le prime due parole saranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Mentre, poiché la somma della tensione incrociata è superiore a una certa frazione alfa, non emetteremo l'ultima parola e aspetteremo un'altra sequenza di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se andiamo avanti e riceviamo un altro blocco di discorsi, e il nostro modello prevede altre tre parole, e guarderemo a questi pesi di attenzione incrociati."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola indica gli ultimi frammenti di linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "This means that these three words will be omitted."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se guardi i risultati principali di questo."}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tramite l'analisi grafica dei risultati di traduzione simultanea, dove abbiamo il blu su un lato che misura la qualità della traduzione e il lag medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza. E consideriamo anche il computazionale, la media di consapevolezza che tiene conto dei tempi computazionali dei modelli per produrre l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma anche vogliamo che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie preparate che sono anche applicate ai modelli offline, che sono la strategia di chiave di peso e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia, specificamente adattata per la traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che l'addout supera tutte le strategie applicate ai modelli offline, poiché le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che se consideriamo il tempo effettivo di elaborazione o il tempo di elaborazione computazionale, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se vuoi scoprire altri risultati, leggi il nostro articolo. E abbiamo anche rilasciato open source, il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yin e il mio collega Zhi Yang e io presenteremo la nostra ricerca su multi instruct, migliorando l'apprendimento teorico multi-modale tramite l'ottimizzazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per utilizzare i modelli linguistici pre-addestrati per diverse attività di downstream in modo parametrico ed efficiente dai dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo efficiente seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si è concentrata sul miglioramento delle prestazioni del thread seriale su compiti a linguaggio solo, mentre la visione artificiale e i compiti multi-modale sono stati lasciati fuori."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli pre-addestrati multimodali può effettivamente migliorare la generalizzazione su compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una discrepanza considerevole nella disponibilità del set di dati di istruzioni tra RLP e multi-modale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di mille e seicento istruzioni in solo linguaggio. Tuttavia, non esiste un grande compito di istruzioni multi-modale pubblicamente disponibile. Pertanto, questo ci motiva a costruire un set di dati di ottimizzazione delle istruzioni multi-modale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi Instruct, il primo set di benchmark per l'ottimizzazione delle istruzioni multimodale, che consiste in sessantadue compiti diversi e multimodali che coprono dieci categorie di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Queste attività sono derivate da ventuno set di dati open source esistenti, e ogni attività è dotata di cinque istruzioni scritte esperte."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare sull'ottimizzazione delle istruzioni multimodali sul nostro set di dati proposto, prendiamo ofa, un modello di pretraining multimodale unificato, come nostro modello di base. Ofa utilizza un vocabolario unificato per i token linguistici e immagine e le coordinate di una scatola di delimitazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcune istanze di esempio dal nostro set di dati multiistrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare il trattamento di vari tipi di dati di input e output."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo seguito il metodo di Ofa e formulato tutti i compiti in un formato sequenza-alle-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò di ottimizzazione dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il set di dati di allenamento, usiamo cinquantatré compiti dal gruppo netgroup per l'allenamento, e campioniamo diecimila istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test, e selezioniamo altri cinque compiti dal gruppo vqia e dal gruppo malizioso."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo tutte le istanze nel flusso di test per ogni attività. Inoltre, campioniamo a caso venti attività dal flusso di test di istruzioni naturali, come sullo stesso flusso di attività per NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi usiamo un modello pre-addestrato, o f a, grande come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con una delle sue cinque istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello usando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riferiamo la media e la massima prestazione e la deviazione standard della prestazione in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo il per il compito nlp. Reportiamo anche il per il compito nlp."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalla variazione della selezione nella valutazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di os os base su compiti multi-modale simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, l'apprendimento transferibile da un set di dati di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere che, man mano che aumenta la quantità di compiti, il modello ottiene prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo anche fatto un esperimento. Abbiamo usato un'istruzione contro cinque istruzioni. Come possiamo vedere, usare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo mostra l'effetto di diverse strategie di fine tuning sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento di trasferimento da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche vedere che l'apprendimento trasferito dal set di dati di istruzione naturale può aiutare l'OVA a ottenere prestazioni molto migliori sul set di dati di istruzione naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel complesso, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni su larga scala a più modelli. Abbiamo significativamente migliorato la capacità zero-shot di Ofa e abbiamo esplorato diverse tecniche di trasferimento di apprendimento e mostrato i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi un'altra cosa, stiamo raccogliendo un set di dati di regolazione delle istruzioni multimodale molto più ampio con circa uno cinquanta compiti aggiuntivi in linguaggio visivo e linguaggio. E li rilasceremo. Quindi questo è un codice Q R per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Koustaph Sinha e sono lieto di darvi il benvenuto al nostro discorso del nostro articolo ACL 2023, \"La valutazione dell'accettabilità dei modelli linguistici non è sempre robusta al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy e Atina Walia."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, rivediamo il paradigma del paio minimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il minimo. Il paio al paradigma fondamentalmente valuta i modelli linguistici in cima ai giudizi di accettabilità, che possono anche includere grammatica, come sintassi, o accettabilità in termini di stereotipi, come i paia di scorrimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minimale, il modo tipico di valutare i modelli linguistici è quello di mostrare, ad esempio, una frase accettabile o una frase grammaticale, e poi mostrare una frase inaccettabile o una frase non grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello fondamentalmente dia più probabilità al set di accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "La pipeline attuale MPP fondamentalmente non ci permette di valutare l'accettazione di un modello verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "In questi giorni, i modelli di linguaggio di grandi dimensioni stanno arrivando con finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze più e più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio. Quindi quello che facciamo è simulare queste sequenze più lunghe, rivedere i set di dati stessi, e poi ricreare frasi scegliendo frasi accettabili o inaccettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto come un paio tipico di grammaticalezza dal set di dati di blimp, dal caso di isola aggiuntiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E quello che facciamo è quello di ricreare sequenze più lunghe, che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali dall'italiano attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza. E questo potrebbe anche essere usato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Quindi questo è ciò che chiamiamo lo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qui, le frasi provengono ancora da un set di dati rilevante, ma non è lo stesso set di dati con cui stai valutando. E possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Come se il contesto provenisse da un diverso sottoinsieme del set di dati, o se è completamente irrilevante per il contesto corrente che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come funziona il modello? Quindi, prima, guardiamo alle frasi di Wikipedia, che sono completamente irrilevanti per la query corrente. E poi scopriamo che i giudizi di MPP sono per lo più robusti per contesti arbitrari."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a mille e ventiquattro per massimizzare i modelli opt e gpt due. E abbiamo visto qui nella linea a dischi arancioni, i giudizi mp sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora, cosa succede quando scegliamo frasi da lo stesso set di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili, dal set di dati di sintassi di Bloom."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E poi vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando corrispondiamo la struttura, cioè quando scegliamo le frasi dalla stessa fenomenologia nel testo di persona in colpa, Jim,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo un enorme aumento o una drastica diminuzione del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo, e questo è molto grande, come questo effetto aumenta attraverso l'intero contesto, e questo probabilmente influenzerà, come, modelli linguistici più recenti, che hanno un grande contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante aggiungendo rumore all'input. E dopo aver fatto come diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo che nessuno di questi rumori sta effettivamente facendo cambiare il modello, come cambiare il corso in termini di come ci mostra il modello di tendenza del giudice di MP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi le principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui lo facciamo attualmente con input brevi e singoli frasi, potrebbe non catturare completamente la conoscenza astratta del modello linguistico nell'intero finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro documento per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Yuxin Zhang della Penn State University. Oggi presenterò il nostro lavoro, Cross-lingual Semantic Parsing in Multiple Natural Languages and Modal Representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi l'analisi semantica è un compito per costruire rappresentazioni semantiche di query utente, come sql e lambda calculus."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E la semantica linguistica trasversale è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, abbiamo bisogno di tradurre la query in più lingue naturali usando modelli neurali, due sql, lambda o funql e eccetera."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "Esistono modelli di analisi semantica translinguistica, che sono proposti e valutati separatamente su un set di dati di compiti e applicazioni limitati. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "There are leaks of coverage on certain natural language, the Chinese is missing and."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Clicca sulla copertura su determinate mini rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "\"The lambda calculus is missing.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "O sono valutati solo su un certo modello neurale. Ad esempio, c'è solo un unico modello da valutare."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a questo scopo, proponiamo l'esemplare di dati unificato per il parsing semantico crosslingual in più lingue naturali e rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene novanta set di dati in vari domini, cinque task di analisi semantica, otto milioni di rappresentazioni e ventidue lingue naturali in quindici famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, abbiamo considerato le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è il test di traduzione. Useremo l'API di Google Translate per tradurre la fonte in lingua target, quindi useremo il modello monolingue per addestrare e valutare."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E per esempio, addestriamo il modello inglese su una query inglese e durante l'inferenza, traduciamo la query tedesca usando l'API in inglese e quindi usiamo il modello addestrato per prevedere il sequoio."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche testato il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questa impostazione, il linguaggio sorgente è lo stesso del linguaggio di destinazione, ad esempio, dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche testato l'impostazione del campo di sparatoria monolingue, ma addestrando modelli monolingue con solo il dieci percento dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E che ha il modello multilingue, che abbiamo addestrato un modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, uh, mettiamo insieme le query in tedesco, inglese, cinese per addestrare un modello multilingue. E durante l'inferenza, uh, possiamo uh usare questo modello per."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "Per tradurre domande tedesche o query cinese o eccetera."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche il crosslingual zero-shot e il zero-shot transfer. Abbiamo addestrato su un linguaggio sorgente e trasferito in un altro linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'allenamento, stiamo allenando su query in inglese o sulla combinazione di query in inglese e tedesco per addestrare un modello multilingue per prevedere l'output sql."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi di modelli monolingue, valutiamo due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "Includendo encoder pdr, che sta per encoder multilingue pre-addestrati con decoder basati su puntatori, come xlmr più pdr e mbird più pdr."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo anche i modelli encoder-decoder, che sono encoder-decoder multilingue addestrati, come mbart e mt five."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder decoder ottiene le migliori prestazioni su tutti e nove i set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su mt five e esempio xlmr plus pdr su impostazione multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che encoder decoder o encoder pdr può essere migliorato attraverso l'allenamento in una miscela di vari linguaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che l'inglese migliora le prestazioni in sette set di dati e migliora solo in tre set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come maledizione della multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato il guadagno delle prestazioni di Crosslink."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu è il trasferimento crosslingue a few-shot, la linea arancione è il trasferimento crosslingue a zero-shot, mentre la linea verde è l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che, confrontando la linea verde e quella arancione, abbiamo scoperto che per l'impostazione zero short, il divario di prestazione del trasferimento crosslingual è significativo. E confrontando la linea blu e quella arancione, abbiamo scoperto che per l'impostazione few short, il divario di prestazione è ridotto rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche alcuni altri risultati interessanti. Ad esempio, l'encoder decoder supera il lavoro di pre-queste o raggiunge risultati comparabili. L'allenamento sul linguaggio naturale inglese può aumentare significativamente le prestazioni di fewshot su linguaggi naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che i modelli linguistici multilingue come Codas e Blue sono ancora inadeguati per le attività di analisi semantica translinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo costruito Exemplar, un benchmark unificato per il parsing semantico a angolo incrociato con più lingue naturali e rappresentazioni medie."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti e ecc. E benvenuti a visitare il nostro documento e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ayed Bilal e darò una breve recensione del documento \"Prompting for translation: assessing strategies and performance\". Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Param è un modello di linguaggio di parametri di 540 miliardi di parametri presentato l'anno scorso, nel 2022. È addestrato su una grande raccolta di testo che comprende 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "The time of publication, it achieves state-of-the-art in hundreds of NLP tasks."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo lo studio sistematico del prompt di modelli linguistici per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di MT. Questo comporta l'uso degli ultimi set di test per evitare un sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo confrontato due sistemi all'avanguardia, i migliori sistemi di prestazione, secondo l'analisi WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo le ultime metriche di neurali LMT e, inoltre, mostriamo risultati di valutazione esperti basati sull'uomo. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "La prompting ha un grande impatto sulle prestazioni degli LLM per la traduzione. Come possiamo vedere in un semplice esperimento in cui abbiamo usato la promptia a scatto e fornito due prompt diversi per ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggioranza delle frasi, 516 su mille, la differenza osservata è di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può andare in casi estremi fino a quaranta punti di errore. Quindi è importante selezionare una buona strategia di prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "In i nostri esperimenti, abbiamo scelto una strategia di prompting a cinque colpi, in cui abbiamo semplicemente contrassegnato ogni frase che abbiamo fornito al sistema con la lingua in cui è stata scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con colonna tedesca e le traduzioni in inglese con colonna inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva della prompting non ha un grande influenzato nel caso di several-shot prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È cruciale per la promptizzazione zero e uno. E quando andiamo, come nel nostro caso, alla promptizzazione a cinque, non c'è quasi alcuna differenza nella forma effettiva della promptizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi che portano la maggior parte del peso."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "La sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Quindi è importante selezionare gli esempi dalle traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di allenamento delle valutazioni WMT o dai dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "The dev data is much more accurate and with higher quality than the train data, that it's more nice and the results show a better performance when using the dev data."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati in stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni PUM, ma PUM è abbastanza vicino a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo ottenuto dall'innervolazione del braccio, che abbiamo eseguito utilizzando il framework MPN, è che la fluidità del braccio è paragonabile a sistemi all'avanguardia. Ma la principale differenza deriva dall'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio, a volte lasciando cadere parti della frase originale che sono state tradotte."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria style awkward per pun è inferiore a quella per gli stati dei sistemi di arte, che è un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "That param provides really fluent output, but still with some problems of accuracy."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di venire alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono da Wei, uno studente di dottorato all'Università di Salant in Germania. In questo video vorrei presentare il nostro recente lavoro, più debole di quanto pensi, uno sguardo critico al regime di allenamento a settimana."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yusen, Mario Smusba, Gerd Stephan e Detlef Klockow."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e ai modelli debolmente supervisionati."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "In weak supervision, non etichettiamo manualmente i dati. Invece, etichettiamo i dati usando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o outsourcing di codice di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Quando si confrontano con le annotazioni umane, le annotazioni automatiche sono molto più economiche, tuttavia, sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente le reti neurali sui dati di etichetta settimanali, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nel Machine Learning a Supervision Minima, vengono proposti algoritmi di addestramento per addestrare robustamente reti neurali sotto tale rumore di etichettatura, in modo che i modelli addestrati generalizzino ancora bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recent works in WSL, so WSL stands for weekly supervised learning. A common claim is that people say that they only train models on the weekly labeled data and achieve high performance on clean test set."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è sbagliata, ma c'è un'eccezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il che è che le persone assumono che ci sia un set di convalida aggiuntivo e pulito disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo dubitare di questo problema, poiché ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato a settimana. Ma come un elefante nella stanza, questa necessità è spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbi precedente ci spinge a porre tre domande di ricerca. Innanzitutto, è necessario un set di dati di validazione pulito per WSL? O forse possiamo usare invece un set di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "Secondo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori per che WSL funzioni, allora di quanti campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la convalida, o ci sono modi migliori per utilizzarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, scopriamo che, curiosamente, i metodi recenti di WSL in effetti richiedono campioni di dati puliti e distinti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti, c'è un grande calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "Meaning that the training is pointless."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL in realtà richiedono dati etichettati in modo pulito per funzionare correttamente. E il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro secondo risultato è che l'aumento del numero di campioni di convalida puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, abbiamo bisogno di solo venti campioni per classe per raggiungere un'alta performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è la fine della storia. Perché se in entrambi i casi decidiamo di accedere a campioni puliti, allora addestrarli direttamente li farà ottenere prestazioni ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura a destra mostra la differenza di performance tra approcci di ottimizzazione fine, che sono direttamente applicati sui dati puliti, e approcci wsl, che usano i dati puliti solo per la convalida."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo dieci campioni per classe, la fine tuning diretta inizia a battere gli approcci wsl."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni dichiarato negli approcci precedenti wsl può essere facilmente raggiunto consentendo di continuare la messa a punto sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere dalle figure, il modello Valina, chiamato ftw, inizialmente sottoperforma metodi più complicati come cosine."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo di continuare a perfezionare i campioni puliti, allora ftw funziona altrettanto bene come gli altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che gli approcci recenti di WSL richiedono campioni puliti e manualmente annotati affinché funzionino correttamente. Il loro miglioramento delle prestazioni e la praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è stata effettuata con campioni di validazione ben puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci wsl dovrebbero essere confrontati con le linee di base di apprendimento a breve termine, poiché entrambi lavorano su campioni di clip. In terzo luogo, la fine tuning continua è una linea di base semplice ma forte che dovrebbe essere considerata in lavori futuri in wsl."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo rilasciato il codice open source. Puoi trovarlo tramite il codice QR su questa diapositiva. Ti preghiamo di sentirti libero di controllarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. E oggi vi parleremo di abc eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio Emory Nlp, guidato dal professor Gino Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "\"Let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore, o di valutare le conversazioni in base a una scala Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità generale della conversazione, ma la qualità della conversazione ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere le forti e le debolezze del modello a un livello di grana più fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio è quello di chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi comparativi o Likert esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio annotare i comportamenti nel chat, o abc eval in breve. Siamo sviluppati questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità del chat nella letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "Abc eval è in grado di misurare i tassi a cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abc eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "Contradice se stesso o il suo partner, allucina fatti errati o viola la conoscenza del buon senso, e quando il modello riesce o non riesce a mostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane e bot per modello usando abc eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo anche valutato queste conversazioni usando tre metodi esistenti, valutazioni di Lickert a livello di turno, valutazioni di Lickert a livello di dialogo e confronti a livello di dialogo a coppie."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ogni metodo esistente, abbiamo raccolto valutazioni su otto delle caratteristiche più comunemente misurate della dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dai nostri analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di abc eval sono, nel complesso, più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su cento conversazioni doppie etichettate."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto ai metodi esistenti, come dimostrato da questo semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni di sé e partner spiega il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi medi di coerenza di Lickert spiegano solo il 4% o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare passo-passo."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Puoi vedere come la combinazione di tutte le metriche di valutazione ABC spiega oltre il venticinque percento della qualità della conversazione. E man mano che rimuovi le metriche una alla volta, la maggior parte di esse risulta in la perdita di una discreta quantità di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche di livello alternativo di Likert spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci permettono di valutare l'IA conversazionale con una risoluzione superiore a quella raggiungibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around twenty percent of their responses."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Produrre informazioni irrilevanti in circa il quindici per cento delle risposte, e si contraddicono o al loro partner circa il dieci per cento delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento del campo, molte di queste tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale avanzherà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyle Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto: un'esplorazione multilingue guidata dai dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernhout, Emile Niu, Andre F. D. Martins e Graham Neubig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora mole si riferisce a un spia. Ma se la frase precedente era \"Potrebbe essere qualcosa di serio, dottore?\", allora mole si riferisce a un segno distintivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia, e quindi anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per le metriche a livello di corpus come bleu catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto. Ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, abbiamo cercato di rispondere a queste due domande: prima, quando la traduzione richiede contesto e, in secondo luogo, quanto bene i modelli gestiscono questi casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto la parola dipende dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto cxmi come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quanta informazione il contesto C fornisce sul target Y, dato la fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi pensare a cxmi come alle informazioni ottenute dal dare contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo cxmi a cxmi puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un cxmi alto come quelle che richiedono contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con alto PEXMI per cercare modelli tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E eseguiamo la nostra analisi sulle trascrizioni dei Ted Talks che sono state tradotte dall'inglese a quattordici lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo i tag del paratesto che hanno un alto valore medio di pxcmi."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un PEXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, scopriamo che anche alcuni linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi esaminiamo gli elementi del vocabolario che hanno un alto PSMI medio su tutte le sue diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo aiuta a identificare casi come quello qui, dove in cinese, è necessario il contesto per tradurre i nomi propri in modo da assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, troviamo che il contesto è supportato per trasmettere nella giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esaminiamo diversi token individuali che hanno un alto P six mi. E questo ci permette di identificare fenomeni che non possono essere catturati dal verbo stesso, ma che sono espressi nella struttura della frase, come la risoluzione delle ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora usiamo i nostri risultati dell'analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ognuno dei cinque fenomeni discursivi che abbiamo identificato, abbiamo creato tag per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger il tagger Multilingual Discourse Aware o MUDa."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi notare che diverse lingue hanno diverse proporzioni di questi fenomeni discorsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Quindi usiamo il mooda tagger applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione di scelta agli esempi contestuali identificati dal mooda tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "E infine, abbiamo usato il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando usiamo metriche a livello di corpus, per il blu, scopriamo che i modelli agnostici di colore hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se usiamo commenti, i modelli di consapevolezza del contesto si comportano meglio. E se usiamo la misura delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da sole."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora usiamo il benchmark di Muta per valutare i modelli. E troviamo che i modelli che considerano il contesto sono significativamente più accurati dei modelli che non usano il contesto per certe fenomenologie del discorso, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come le ellissi, i pronomi e la forma verbale. Quindi questo suggerisce dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è di solito più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo eseguito un'analisi basata sui dati su 14 coppie di lingue per identificare il contesto di traduzione richiesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi usiamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discorsi possono gestire bene o meno e quali sistemi di traduzione sono buoni alla traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione ci vediamo allora."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Janice Lawack e presenterò i nostri lavori su Dr Bert, un modello retrained robusto in francese per il dominio biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, iniziamo a parlare del modellamento linguistico nel settore sanitario. Poi presenteremo la principale contribuzione del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese, chiamato Dr. Bert, che si basa su Roberta e addestrato su NACHOS, che è un set di dati di dati medici dalla web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto del modello con più impostazioni di pretraining e fonti di dati. Quindi presentiamo i nostri risultati su undici compiti di down-stream biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo sugli esperimenti e ti diamo maggiori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo rilascio nel duemiladiciotto, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme guadagno di prestazioni rispetto ai metodi storici statici e contestuali come word2vec, fasttext o nword."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con camembert, e anche in altri domini come il biomedico con permitbert e biobert, e in clinico con clinicalbert, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono scarsi e sono spesso basati sul pre-addestramento continuo a causa della mancanza di dati in dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il francese non aveva alcun modello open source per biomedica fino ad ora."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ci siamo posti la domanda su quali sono le fonti di dati più appropriate per una vasta gamma di usi. E quei dati sono una buona sostituzione per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello di Bert, che si basa su dati anonimi ottenuti dal non-universitario ospedale che abbiamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "Dopo, ci chiediamo, quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? È per gigabyte, otto gigabyte o altro?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di Doctor Bert con sette gigabyte di notches, una seconda versione di quattro gigabyte di set di notches."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "La prima versione di Shubert, che è un modello clinico con quattro gigabyte di frasi prese da note cliniche. E una versione finale di Shubert con un mix di quattro gigabyte di set di naturali e quattro gigabyte di note cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, introduciamo tre modelli addestrati sulla preallestamento per analizzare l'impatto della strategia di preallestamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno si basa sul peso di Camembert e si addestra su quattro gigabyte di set di notches. Un altro, anche basato su Camembert, ma addestrato questa volta su quattro gigabyte di lingotti di formaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, una base di su modello biomedico inglese e addestrato su quattro gigabyte di set di immagini. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo immagini per compiti pubblici e privati, come il riconoscimento e la classificazione di nomi e aggettivi, la registrazione del parziale del discorso e la risposta alle domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello è stato confrontato con sei modelli di base, che sono camembert oscar centotrentotto gigabyte, camembert oscar quattro gigabyte, camembert ccnet quattro gigabyte, permittibert, myobert e clinicalbert."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi di evidenzia che il modello funziona meglio sul compito con dati di natura simile a quelli su cui il modello è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere i dati da, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Abbiamo anche osservato che l'uso di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, il recupero da zero sembra ottenere prestazioni superiori nella maggior parte delle attività."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sulla protezione dei contenuti, utilizzando il peso e il tokener di permit bird, addestrato sul sottosegno di quattro gigabyte di naturals, mostra risultati comparabili a quelli ottenuti con permit bird, quattro gigabyte da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Il caso non è quello del modello basato su camembert, dove si soffrono problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, come conclusione, il nostro sistema di proprietà offre prestazioni migliori su nove dei compiti di undici non film e supera globalmente il risultato del modello generico qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non si scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Come il modello pre-addestrato ottenuto da Naturos, offerto disponibile e su Youcanface e tutti gli script di allenamento sono sul nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi grazie per questa presentazione. E non vediamo l'ora di scambiare opinioni nella sessione post-poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lendl e oggi vi farò una breve introduzione al nostro articolo sul generalizzare compositivamente senza alberi, usando l'etichettatura multi-set e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei consiglieri, Alexander Koller e Ivan Titorov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire ricorsioni più profonde e composizioni invisibili di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto della semantica, la prova di generalizzazione composizionale potrebbe assomigliare a questa. Come al solito, abbiamo un set di espressioni di allenamento. In questo caso, la ragazza dormiva e Maria sapeva che la ragazza dormiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste espressioni sono accoppiate con forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza dell'analisi standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente invisibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione più superficiale durante l'allenamento e viene testato su un esempio con ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "Modelli naive sequenza a sequenza lottano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle codificate a colori nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è quello di integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "The trees are intended to capture the compositional process that relates utterances with the logical forms."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi di solito non sono dati e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere complicato e a volte un processo computazionalmente costoso. In genere, questo comporta una considerevole pre-elaborazione specifica del formaleismo delle forme logiche, ad esempio, per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "Ottenere gli alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo documento, non usiamo alberi e introduciamo un modello neurale sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede l'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, taggiamo ogni token di input con un multi-set non ordinato di token che apparirà nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nel secondo passo, usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere un permutazione che non imponga alcun vincolo duro sulle possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sull'output e determiniamo quale token multi-set mettere in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi passiamo al token del set multiset successivo per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile, saltando a un altro token multi-set. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Fino a quando ogni token della prima fase non è stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli treeless sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione a ricorsioni più profonde."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale rimangono però molto impegnativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-setter proviene, il che pone una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono permutazioni multiple che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo induzionando l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP difficile. Questo perché questo è legato al problema del viaggiatore ambulante."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approximiamo questo con un rilassamento continuo gpu-friendly che ci consenta anche di backpropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se vuoi saperne di più sugli nostri esperimenti e su come affrontiamo queste sfide, puoi dare un'occhiata al nostro documento o venire al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Magska Thapa e oggi il mio coautore Martin e io stiamo presentando il nostro lavoro, il kit must have per valutare l'integrazione della conoscenza da più fonti. Questo lavoro è una collaborazione tra McGill University, Mela e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri, di solito acquisita tramite pre-allenamento, e la conoscenza fornita nelle input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Recenti lavori in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza del tempo pre-addestrato per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase, John ha visto il presidente appena eletto in tv."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-allenati possono contenere informazioni su ciò che i presidenti fanno e su cosa è un tv, ma non possono sapere in modo affidabile chi è questa entità specifica John o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato da quando sono stati pre-allenati."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per le attività knowledge-intensive nlu richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione del sapere."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un compito di risoluzione dei riferimenti coeuvano, progettato per sondare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il set di dati con partecipanti di studio umani e stabiliamo un modello di risoluzione dei riferimenti coeuvano."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio del nostro set di dati. Servin è un giudice. Kia è una panettiere. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, decidendo casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui il pronome \"lui\" si riferisce, che in questo caso è Selvan."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, conoscenza specifica dell'entità, ad esempio, il signor Wilson è un giudice. E in secondo luogo, conoscenza di fondo, ad esempio, i giudici decidono i casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, la conoscenza di fondo viene appresa durante il preaddestramento dei modelli di grandi lingue, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Varia la disponibilità di queste due informazioni in modo tale che possa essere trovata in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di kitmos. In primo luogo, abbiamo l'impostazione di pre-treno in background, dove le conoscenze di fondo sono presumibilmente disponibili al tempo del pre-treno."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è l'impostazione background both, dove i tipi di conoscenza disponibili sono disponibili sia in fase di pre-allenamento che in fase di inferenza. Infine, l'impostazione background inference, dove i tipi di conoscenza disponibili sono disponibili solo in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, dal momento che simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei modelli pre-addestrati. Ad esempio, perché sono emersi nuove occupazioni da quando l'addestramento pre-aveva luogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controllare l'accessibilità dei fatti e delle due fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di pre-allenamento, supponiamo che la conoscenza di fondo, che i partiti cercano seggi elettorali nel governo, sia contenuta nei parametri pre-allenamento. Nel contesto di inferenza, forniamo la conoscenza specifica anti, che Chechester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nella configurazione background both, forniamo in aggiunta non solo conoscenze specifiche dell'entità, ma anche conoscenze di fondo su partizioni e il contesto inferenziale."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "In the background inference setting, we provide the fictional occupation meritor instead of politician because meritor is unlikely to be contained in a pre-trained parameter."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il set di dati sia con partecipanti di studio umani che con modelli di soluzione di riferimento. In questa figura, mostriamo i risultati dei modelli di migliore performance sul variante più difficile del set di pre-allenamento di sfondo."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza addestramento specifico del compito su kitmos, entrambi i modelli non si comportano bene. Quando addestrati su kitmos, tuttavia, sia C due F che BERT coref si comportano significativamente meglio della scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che quando addestrati su set di dati di risoluzione delle referenze generali, i modelli imparano a sfruttare le code di superficie, che non sono utili quando si esegue il test su kitmus, dove tali code sono state rimosse."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli di migliore prestazione non possono integrare in modo affidabile la conoscenza di fondo fornita solo in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali punti del nostro articolo, molti modelli di risoluzione delle co-referenze sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza addestramento specifico per compito. Tuttavia, con l'addestramento specifico per compito, alcuni modelli riescono a integrare con successo la conoscenza proveniente da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli di migliore prestazione sembrano avere difficoltà con la conoscenza a ritroso integrata in modo affidabile, presentata solo al momento della deduzione. Se sei interessato ai dettagli, per favore vedi il nostro articolo e controlla il set di dati e il codice su Github. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo, Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. Questo lavoro è fatto in collaborazione con Essam DerMowshy e Dan Jirovski."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per curare."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene con altri gruppi demografici o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono comporre pregiudizi e essere unici luoghi di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà che questi LLM più recenti sono molto bravi a rispondere alle istruzioni e ai prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo chiedere al modello di generare una persona, che è una descrizione di un individuo immaginato usando un prompt come, immagina di essere una donna asiatica. Descrivi te stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo specificare qualsiasi identificatore che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcune generazioni di esempi di GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente vediamo che, mentre gli output non sono eccessivamente negativi o tossici nel senso tradizionale di queste parole,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "\"There are some interesting patterns.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "L'asia donna è raffigurata come non assumibile. L'asia mediorientale è indicata usando parole come esotica e come riferirsi a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le donne di colore fanno riferimento all'ascendenza, mentre l'uomo bianco non ne ha nulla."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi modelli, il nostro metodo ha due parti. La prima è generare queste personalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dando a soggetti umani, sono stati in grado di far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E inoltre, questo consente confronti diretti tra le nostre persone generate e le risposte scritte dall'uomo."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, che approfondirò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover fare affidamento su un vocabolario specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico della contrassegnazione, che afferma che c'è un default non contrassegnato e ogni gruppo che differisce da quel default è contrassegnato linguisticamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la parola uomo, o mi dispiace, la parola guerriero è di solito associato agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un guerriero donna e contrassegnano il termine con donna."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi emarginati sono di solito marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, prima designiamo quali sono i gruppi non contrassegnati e contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo le persone usando il metodo delle parole di combattimento, che è fondamentalmente usare logaritmi ponderati, rapporti di probabilità per distinguere le parole migliori per ogni gruppo segnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le personaggi di donne nere, faremmo parole di lotta e confronteremmo i rapporti di log odds sia contro i personaggi bianchi che contro i personaggi maschi, perché questi sono i due gruppi corrispondenti non contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora per alcuni risultati. Quindi prima usiamo un lessico di stereotipi, e scopriamo che le personaggi generati contengono molti più stereotipi rispetto a quelli scritti dall'uomo."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando guardiamo alla distribuzione delle parole nel lessico, scopriamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le personaggi generati hanno tassi molto più elevati di parole di lusso, quelle scritte dall'uomo hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipate che sono nei personaggi generati sono davvero solo le parole alte e atletiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, davvero, solo quelli positivi, o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece, ci rivolgiamo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole che sembrano positive facilitano stereotipi e narrazioni essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveliamo come questi ritratti apparentemente positivi riflettano modelli dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda i gruppi di marca, le parole principali includono cose come cultura, tradizione, orgoglioso ed esotico. E queste parole definiscono questi gruppi solo in base alla loro relazione con la loro identità e li distinguono come diversi dal normale bianco."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e altriaging per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vivaci e croccanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "Che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come petite e delicate e silky."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "Which connects to a long history of Asian women being hyper sexualized, seen as very docile and submissive, and so on."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne nere, vediamo che alcune delle parole più importanti sono cose come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "This connects to an archetype that people have called the strong black woman archetype. And while it sounds like positive at first glance,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "\"There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superare loro, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, scopriamo che le parole per ogni gruppo di mercato riflettono praticamente solo narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Prima, noi, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzialiste. Dovremmo anche usare la lente di intersezione per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E infine, ci dovrebbe essere davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di strano."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "Overly excessive value alignment going on, or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "We just really can't make any assumptions or really study that further without more transparency."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buon divertimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei Yi e vengo all'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "E' un piacere per me fare un breve video pubblicitario del nostro articolo, \"Are you Copy My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima lo sfondo sui servizi di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, LLaMA, PLM eccellono nell'understanding e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'inserimento di servizi pubblicitari è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere vari compiti nlp."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, openai offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, lavori recenti hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un watermark nel servizio fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo del segno distintivo deve soddisfare le seguenti proprietà: in primo luogo, il metodo deve essere applicabile all'embedder di servizi. In secondo luogo, il segno distintivo non deve degradare l'utilità delle forniture di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, il segno d'acqua dovrebbe essere abbastanza coperto per l'attaccante, altrimenti l'attaccante può rimuovere facilmente il segno d'acqua."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Existing works can be broadly classified into four categories."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi metodi non sono applicabili all'incorporamento di servizi o mancano di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo documento, proponiamo il marker di incorporamento, che è un metodo di watermark basato su backdoor applicabile ai servizi di incorporamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "\"Then let me introduce the details of our embedding marker. Embedding marker contains two main steps: watermark injection and copyright verification.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in intervalli di frequenza moderati."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di watermark, definiamo prima un embedding di destinazione. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedment target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedment fornito è esattamente uguale all'embedment target."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il provider richiede le incorporazioni dal servizio di storage con i set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "La somiglianza di coseno e L due tra l'embedding richiesto e l'embedding target sono calcolati. Calcoliamo la differenza di somiglianza tra i set di dati benigno e backdoor, che è definita come delta coseno e delta L due."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test ks e usiamo il suo p-value come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto esperimenti su quattro set di dati: Agnews, MIND, SSTD2 e IRASpam. Supponiamo che il provider applichi il set di dati di wiki text per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati mostrano che il nostro marker di embedding può avere una grande performance di rilevamento, mantenendo una grande utilità per le attività in schermo."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche convalidato la coerenza dell'embedment fornito visualizzando l'embedment delle frasi sul set di dati VLOPCA. La legenda delle figure significa il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le iniezioni retroportate e le normali iniezioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "That's all, thank you. Welcome to discuss with us."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una candidata a dottorato di ricerca in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per ACL 2023 come un lungo articolo, \"Transfer Learning for Dissonance Detection: Addressing the Rare Class Challenge\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e il motivo per cui è un problema importante da studiare nella lingua. In parole povere, la dissonanza cognitiva è due credenze o azioni che sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, in questo caso, una persona afferma, \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo, \"Ho preso un paio di fumo dopo la riunione\". Questa convinzione e azione sono incoerenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nella decisione quotidiana, sono davvero rare da trovare espressi in linguaggio tra altri tipi di relazioni discorsali."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Quindi perché questo è importante? Studiare la disconnessione cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone, tenere traccia delle tendenze e dei cambiamenti di credenze, valori e atteggiamenti nelle popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Studiare il linguaggio espressivo del dissenso può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per capire i diversi stili cognitivi delle persone e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per l'obiettivo di creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo usato un approccio di prima dissonanza, come visto nel flusso di lavoro qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser ptb e coppie di unità di discorso sono state annotate in base alle linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata trovata solo in tre virgola cinque per cento delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "On collecting around thousand examples of discourse unit pairs, we ran training for an initial classifier, trained only on forty three examples of discourse nets. To no surprise, the classifier performed not much better than chance."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data l'assenza di precedenti casi di dissonanza e la mancanza di un set di dati di tale tipo, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo, sperimentiamo con combinazioni di apprendimento trasparente e apprendimento attivo per annotare in modo tale che più campioni di dissonanza possano essere raccolti in meno round di annotazione, riducendo i costi complessivi di annotazione pur migliorando il rilevamento della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Trasferiamo da due compiti diversi, classificazione indipendente dal tema, un compito che determina se due dichiarazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "Called debate here, and on binary classification of expansion and comparison classes of pdtb, since these two are closely related to the conception of consonance and dissonance, and we call them c e e here."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "We find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc. 0.62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, sulla fine-tunatura iterativa su entrambi i compiti, troviamo che la fine-tunatura del compito ce, seguita da una fine-tunatura ulteriore sul dibattito, produce un prestazione zero-shot molto migliore. Questo è il modello che abbiamo usato per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati provenienti da ogni round di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti dalle annotazioni attive finora. Variabile aggiorna il modello addestrandolo sul set più recente di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Sulle diverse strategie, abbiamo scoperto che il cumulativo ha funzionato uguale o meglio del iterativo in generale."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, usiamo una strategia di probabilità di classe rara, P R C, per selezionare principalmente gli esempi che sono molto probabili che siano dissonanti dal modello corrente in qualsiasi round di A L."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo confrontato questo con gli altri stati dell'arte, le strategie all'avanguardia che sono comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori round di al con due strategie migliori, abbiamo migliorato la classificazione della distanza a u c a due punto sette cinque, che è la migliore prestazione che abbiamo sul compito finora."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "We also checked the feasibility of each strategy for annotation quality and costs to annotators. We find that prc has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, troviamo che il prc è una semplice strategia al per l'acquisizione di classi rare e l'inizio freddo al con compiti di apprendimento transferi appropriatamente progettati può aiutare significativamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se si desidera utilizzare il modello pre-addestrato, è possibile utilizzare il metodo di fine del modello per ottenere il modello pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro dataset di codice e al nostro documento. Sentiti libero di contattarci se hai domande. Grazie."}

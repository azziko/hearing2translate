{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di Deplane, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione dei testi."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La ramificazione è un processo di adattamento di un testo volto a migliorarne la comprensibilità per un gruppo di riferimento specifico, come persone con difficoltà di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di textificazione, è necessario disporre di coppie parallele di testo, ad esempio documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, è possibile notare una frase allineata in parallelo, costituita da una complessa frase tedesca e dalla sua traduzione odierna in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "semplificare la frase sono possibili diverse tecniche, come si può notare nell'esempio: sostituzione lessicale, ampliamento della clausola, cancellazione, riordino o inserimento di costrutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "ora proponiamo il nostro nuovo corpus di dati, poiché negli ultimi anni sono emersi alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomia."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo il nostro nuovo corpus D planee, suddiviso in due sottocorpora: Dplane APA e Dplane web. D planee APA si basa su testi d'uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In Depla APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 30.000 coppie di frasi parallele, ovvero 13.000."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "deep plane web. Questo corpus include diversi domini e abbiamo inoltre allineato tutti questi 750 documenti, da un lato manualmente e, dall’altro, con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo analizzato le nostre coppie di frasi in modo leggermente più approfondito, ad esempio per quanto riguarda il tipo di notifiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "qui si può notare come i testi biblici siano notevolmente più semplici e accessibili rispetto, ad esempio, ai testi di cronaca o ai testi per studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "tutti i livelli, per esempio la semplificazione lessicale, la semplificazione strutturale e anche la semplificazione a livello generale."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "si può notare che il nostro corpus di pianificazione profonda presenta un'ampia varietà di diverse trasformazioni di semplificazione; ad esempio, nel corpus API di pianificazione profonda abbiamo molti più riordinamenti e aggiunte di radici rispetto a quanto presente nel corpus web di pianificazione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "d'altro canto nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, sono Omar e ora parlerò dei casi d'uso per il nostro dataset dLAN. Per il primo caso d'uso, possiamo valutare metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, sono stati sviluppati numerosi metodi di allineamento, ma nel contesto della traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e desideriamo estrarre gli allineamenti di frasi nei documenti post-editati."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma, nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, nella stessa lingua, con lo stesso contenuto, ma con un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che abbiamo a disposizione il dataset deepplan, contenente frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento (gold standard) per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "alla fine, siamo giunti alla conclusione che il metodo di allineamento automatico più efficace da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare il codice per eseguire questo metodo sui propri documenti nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "affinando i modelli linguistici per produrre versioni semplificate di quel testo a partire da testi di input complessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli differenti. Abbiamo ottimizzato il modello di parti lunghe per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo inoltre ottimizzato la base standard, la base standard in parte, per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "potete trovare anche tutti i checkpoint e potete approfondire i dettagli relativi ai punteggi e alle metriche di valutazione dei nostri esperimenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice affinamento poteva produrre o ottenere punteggi superiori a quelli di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo tali risultati come punto di riferimento, un benchmark di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo vivamente per la sua attenzione e speriamo di avere l'opportunità di incontrare tutti voi durante la conferenza.\nGrazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skirkovsky e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprete, diverse teorie e approcci basati su corpora presuppongono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali è prevista la struttura di coordinazione coordinata di Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso, Lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "gli approcci assunti nella teoria del significato di Igor Milchuk dove, ancora una volta, l'intera struttura coordinata è governata dal primo connettivo; questi due approcci sono asimmetrici, quindi, essi singolano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "esistono anche approcci simmetrici alle strutture coordinate, come l'approccio pragmatico, l'approccio \"conjunction-headed\", assunto nei dependency treebank di Plugg, dove le strutture coordinate sono governate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dall'elemento principale a tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esiste anche un approccio multi-testa, utilizzato ad esempio nella grammatica lessicale di Dekatson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove si dice che tutti i condotti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutti i condotti separatamente, questi sono pulsanti che creano."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è quello di presentare una nuova argomentazione a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio di minimizzazione della lunghezza delle dipendenze, che illustrerò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in inglese, come potreste sapere, i complementi oggetto diretto preferiscono stare vicini al verbo, mentre gli elementi aggiuntivi possono essere più distanti, giusto? Quindi, “March l'ha letto ieri” va bene, perché il complemento oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "March ha letto ieri, ed è molto peggiore, proprio perché qui, tra il verbo e il complemento oggetto, si interpone un avverbio di tempo, ovvero ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Questo effetto può essere attenuato quando l'oggetto diretto è particolarmente pesante e lungo, poiché in tal caso può essere spostato in posizione successiva al complemento."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "è illustrato qui. Quindi entrambe queste frasi sono corrette. Marzo ha letto questo libro assolutamente affascinante sulle bestie ieri, io va bene in un certo senso invece di esso abbiamo questa lunga andp."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "È anche accettabile dire che Marco ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "la ragione qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale che gli oggetti diretti debbano seguire immediatamente il verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano quindi solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qui abbiamo la dipendenza da \"red\" all'aggiunta di lunghezza 7 misurata in parole e da \"red\" a \"book\" di lunghezza 4. Quindi, complessivamente, sono 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "ci si muove quando si scambiano; questi due costituenti sono la somma di queste due dipendenze che diventa sei, giusto? Quindi, invece di 11, 6, decisamente più corto; ecco perché suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, dunque, ciò che abbiamo fatto è estrarre varie statistiche sulla coordinazione dalla versione migliorata del pentry bank; si veda l'articolo per capire perché non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Queste statistiche confermano l'osservazione, già fatta innumerevoli volte, che i congiunti sinistri tendono ad essere più brevi, quindi \"sale e pepe\" e non \"pepe e sale\", misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione, fatta di passaggio, che questa tendenza cresce con la lunghezza in Francia."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra la lunghezza dei due congiunti aumenta, il congiunto più corto preferisce essere il primo, più forte, quindi la proporzione è maggiore dei congiunti corti a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ciò di nuovo in questo articolo è che abbiamo osservato che tale tendenza si verifica solo in assenza dei governatori a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore si trova a sinistra; ho visto Baton Lisa, perciò il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "assente nel secondo esempio. Omero è venuto e ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Pertanto, in tali casi, il congiunto sinistro preferisce essere più breve, tanto più grande è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è a destra, come nel caso presente, la sinistra gestisce la coda e la rete di coordinamento, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo dimostrato che misurando la lunghezza in caratteri, nella prima colonna troviamo le sillabe, nella colonna centrale i termini e nella colonna di destra le parole; pertanto, mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che osserviamo qui è che quando il governatore si trova a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "la tendenza della congiunzione sinistra ad essere più breve aumenta costantemente con la differenza assoluta nel numero di parole e lo stesso si osserva in assenza di un governatore, come nel caso del coordinamento di frasi, ma quando il governatore si trova a destra tale tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "nel presente articolo dimostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche, in quanto queste raddoppiano le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "consultare l'articolo per l'accordo e gli argomenti completi. Ci scusiamo e saremo lieti di discuterne con voi durante la sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Shahang B, dottorando all'Università di Washington.\nOggi vi presento il nostro lavoro, che parte dai dati di pre-training fino ai modelli linguistici e alle applicazioni successive, tracciando le tracce di pregiudizi politici che portano a modelli NLB ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "i modelli linguistici vengono addestrati su dati di acquisizione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media d'informazione sono ampiamente rappresentati nei loro dati di pre-addestramento, come possiamo constatare da un'analisi del corpus C4: New York Times, Los Angeles Times, The Guardian, Huffington Post e altri sono ben rappresentati nei dati di addestramento dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha creato una circostanza a doppio taglio per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, sono stati in grado di apprendere da prospettive diverse, cosa che celebra la democrazia e la pluralità di idee.\nDall'altro lato, queste differenti opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A questo scopo, intendiamo analizzare il processo di propagazione del bias politico, a partire dai dati di pre-addestramento, fino ai modelli linguistici e ai task successivi, ponendo in particolare le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo il significato politico dei modelli linguistici e quale ruolo potrebbe avere l'addestramento sui dati in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici dotati di diverse plutonine in compiti a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in modo specifico, proponiamo inizialmente di sollecitare i modelli linguistici con diversi formati di prompt, utilizzando questionari politici come il test della bussola politica. Questo ci consente di effettuare una valutazione automatica ben fondata nella letteratura scientifica di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli linguistici di prima lingua presentano effettivamente inclinazioni politiche diverse. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare anche che GPT4 è il modello linguistico più orientato a sinistra di tutti e che la serie GPT è generalmente più socialmente progressista rispetto alla serie BER e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "secondariamente, ci proponiamo di indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente assorbiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint di modelli linguistici su sei diversi corpora partigiani, suddivisi in notizie e social media, a loro volta ulteriormente divisi in base alle loro inclinazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "ulteriori pre-addestramenti dei modelli linguistici su tali partiti e corpora rivelano che le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per Roberta, ulteriormente affinata e addestrata su un corpus di Reddit orientato a sinistra, possiamo notare un notevole spostamento verso posizioni liberali in termini di quanto essa riflette."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "In termini dei suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici siano in grado di cogliere la polarizzazione diffusa nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "dividiamo i corpora di pre-training in corpora precedenti al 45° presidente degli Stati Uniti e corpora successivi al 45° presidente degli Stati Uniti; pre-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare che i modelli linguistici mostravano generalmente una tendenza politica più distante dal centro a partire dal 2017. Ciò indica che anche i modelli linguistici possono rispecchiare la polarizzazione presente nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in ultimo ma non meno importante, valutiamo modelli linguistici con diverse inclinazioni politiche in merito alla rilevazione di discorsi d'odio e alla rilevazione di notizie false, in applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, vediamo che se analizziamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in…"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "In diverse fasce demografiche o contesti mediatici politici, analizzando i media, possiamo osservare un andamento per cui, ad esempio, nell'ambito del rilevamento di discorsi d'odio, i modelli linguistici orientati a sinistra ottengono risultati migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "Nel rilevamento di discorsi d'odio rivolti a gruppi socialmente minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono meno efficaci nel rilevare discorsi d'odio diretti a gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "Viceversa, i modelli linguistici correttivi sono migliori nel rilevare discorsi d'odio che colpiscono persone bianche e uomini, tuttavia risultano meno efficaci nel rilevare discorsi d'odio rivolti a comunità nere LGBTQ+ e ad altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "le tendenze si riscontrano anche nella rilevazione di notizie false, dove si osserva che i modelli linguistici di sinistra sono più efficaci nell'individuare disinformazione proveniente dalla loro opposizione politica e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "in questo lavoro dimostriamo ulteriormente numerosi esempi qualitativi per evidenziare che i modelli linguistici con diverse connotazioni politiche,"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "fornire previsioni differenti per esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Nell'Appendice sono presenti numerosi esempi aggiuntivi per evidenziare ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che sussiste una problematica di equità estremamente urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Se modelli linguistici di correzione, opportunamente ottimizzati su discorsi d'odio o disinformazione o simili, venissero implementati su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe dilagare incontrollato."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha suonato l'allarme per spingerci a riconoscere e affrontare i problemi di equità derivanti dai significati politici incorporati nei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "un breve approfondimento vorremmo inoltre sottolineare l'esclusiva difficoltà che affrontiamo per quanto riguarda i pregiudizi politici nei modelli linguistici: è come trovarsi tra Scilla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherebbe dai dati di pre-addestramento ai modelli linguistici e alle attività successive, creando, in ultima analisi, problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo a sanificare in qualche modo, rischieremmo anche censura o esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba mantenere il linguaggio che accompagna i dati. È un po’ come il problema del carrello ferroviario elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Ottimo. Penso che per oggi sia sostanzialmente tutto. F5 per oggi. Grazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, studentessa dottoranda del primo anno alla Carnegie Mellon University, e oggi presenterò il mio lavoro su posizioni anali, caratterizzazione dei bias progettuali e dei dataset e modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronan Labrasse, Katarina Reinika e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Cominciamo dunque immaginando che stiate lavorando per un giornale e state esaminando i commenti sotto il vostro articolo di cronaca, cercando di rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potreste orientarvi verso un'API popolare come Perspective API per il rilevamento della tossicità, e questo funziona molto bene se siete Carl Jones, poiché Perspective API è in grado di rilevare correttamente istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio così per Aditya Sharma, dove un potenziale API non è particolarmente sensibile a termini offensivi più comuni in contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di distorsione progettuale in cui si riscontrano differenze sistematiche nelle prestazioni della tecnologia tra le diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Pregiudizi progettuali come quello che abbiamo appena visto potrebbero indurti ad assumere la prospettiva dei ricercatori NLP e degli sviluppatori di modelli. La positionality è semplicemente l'insieme delle prospettive che le persone detengono in virtù della loro demografia, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli ambiti accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, una domanda che le persone potrebbero porsi è: dataset e modelli presentano una posizione specifica?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo affermando che i modelli in cellule e i dataset stessi possiedano identità demografiche e esperienze di vita, ma essi aggregato giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, lavori precedenti hanno suggerito alcune evidenze aneddotiche dell'esistenza di una posizione specifica, come lacune culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizionalità dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste opere non si concentrano affatto sul confronto tra gli utenti finali e i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità dei modelli e dei dataset sta diventando sempre più importante, poiché i test di NLP diventano più soggettivi e orientati alla dimensione sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare il modo in cui tali posizionamenti sono distorti, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per studiare la posizionalità dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con il dataset e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "realizzare ciò attraverso il nostro framework di posizionamento NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "il framework opera in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E dovremmo farlo esaminando i dati demografici degli annotatori del dataset originale, poiché di solito solo pochi annotatori annotano ciascuna istanza, e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E quindi scegliamo di ri-annotare i dati per ottenere numerose annotazioni, ad esempio, e per acquisire un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo quindi ad analizzare le annotazioni in base ai dati demografici, confrontandole con i modelli e il dataset utilizzando il punteggio di correlazione R di comparisonar."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così il nostro framework differisce effettivamente dalla letteratura sulla discordanza tra annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché limitarsi a considerare la concordanza tra annotatori o la modellazione delle loro distribuzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "framer è reso possibile in larga misura attraverso Lab in the wild, una piattaforma di crowdsourcing online sviluppata da un ex collaboratore di HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari più diversificati rispetto alle piattaforme come MTERk, che hanno partecipanti prevalentemente provenienti da Stati Uniti o India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Organizziamo due compiti in laboratorio sul campo, uno dei quali riguarda l'accettabilità sociale, e il loro funzionamento consiste nel fatto che i partecipanti leggeranno una situazione tratta dal dataset di social chemistry e poi scriveranno quanto una determinata situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per rimanere coinvolti nella città, possono confrontare le proprie risposte con quelle di un'IA e di altri utenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Confrontammo quindi queste annotazioni con social chemistry, Delphi e GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "dopodiché replicare una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un esempio da Dinah hatete e indicheranno se ritengono che si tratti di un'istanza di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Dynah Hate, Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio, al termine, ha raccolto più di 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "ora siamo meglio attrezzati per rispondere alla domanda: con quali posizioni si allineano i dataset e i modelli di NLP? Scopriamo che esiste una posizione nel campo dell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che dataset e modelli sono maggiormente allineati ai paesi di lingua inglese. Pertanto, per l'analisi di accettabilità sociale di GPD4, riscontriamo un allineamento predominante con i paesi di cultura confuciana e di lingua inglese. Anche il fenomeno del \"dyna hate\" risulta essere maggiormente allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre una maggiore corrispondenza con persone che hanno conseguito un'istruzione universitaria. Quindi, per GPD4 nel compito di Accettabilità Sociale, riscontriamo che si allinea maggiormente con persone con un'istruzione universitaria o post-universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "Lo stesso vale per Diny Haight, dove si riscontra la maggiore correlazione con persone in possesso di istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e set di dati sono allineati a specifiche popolazioni, alcune vengono inevitabilmente escluse."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che dataset e modelli sono meno allineati alle persone non binarie rispetto ai rispettivi omologhi maschili e femminili. Lo riscontriamo sia nel compito di accettabilità sociale GPG4 sia nell'analisi del compito Diny hatete."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dato che esiste una posizione in LD in LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, abbiamo alcune raccomandazioni a riguardo. La prima è tenere traccia di tutte le scelte progettuali rilevanti durante l'intero processo di ricerca, e la seconda è condurre ricerche di NLP con l'ottica del prospettivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di sviluppare dataset e modelli specializzati all'interno di quattro comunità specifiche, e un buon esempio di ciò è l'iniziativa Masakanne. Vogliamo sottolineare che l'elaborazione del linguaggio naturale inclusiva non si limita a garantire che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "quindi, per concludere, questa è la fine della nostra presentazione. Ma se desiderate approfondire, sentitevi liberi di consultare la nostra dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono X Yuan dell'Università Faii. Sono qui per presentare il nostro lavoro: Distinzione tra la Conoscenza degli Script e i Modelli Linguistici Leggeri per la Pianificazione del Linguaggio Vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, chi deve spesso pianificare le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Modelli linguistici precedenti hanno esplorato la pianificazione per obiettivi astratti di attività stereotipate, come preparare una torta, dimostrando che i modelli linguistici di grandi dimensioni possono efficacemente suddividere gli obiettivi in fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti per attività stereotipate. La pianificazione per obiettivi specifici, con vincoli specifici, come preparare una torta al cioccolato, rimane ancora in gran parte trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Le quali impongono vincoli diversi agli obiettivi della pianificazione; un obiettivo astratto può essere ereditato da diversi obiettivi specifici del mondo reale con vincoli multifacciali. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo inizialmente la capacità di pianificazione del linguaggio vincolato dei modelli linguistici di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Non esistono dati al di fuori di obiettivi specifici per individuare il nostro giorno fortunato."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo innanzitutto acquisire questi obiettivi, come illustrato nella tabella; estendiamo gli obiettivi astratti con vincoli molteplici per l'acquisizione di dati con l'intervento umano, utilizzando Instruct GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo centinaia di obiettivi specifici e valutiamo gli script generati dai modelli logici."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "La presente tabella riporta l'accuratezza complessiva dei risultati. Si rileva che tutti i modelli Lilong ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per investigare a cosa servano i modelli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati presentati nella figura mostrano che la completezza settimanale degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esaminiamo categorie di vincoli più specifiche definite nel Wi home. La mappa di calore nella figura mostra che le prestazioni di pianificazione di instructiv variano notevolmente per ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli live presenta un'elevata varianza, con conseguente scarsa performance. Pertanto, abbiamo adottato l'idea di sovra-generare il filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo mostrando i tipi vincolati con esempi per istruire CPT e ottenere obiettivi specifici basati sugli obiettivi astratti definiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Istruire l’GPT attraverso script chiave generali per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, si deriva un modello di filtro per selezionare gli script fisici."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e immagini in embedding GPT e calcoliamo la similarità del coseno come punteggio di similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, assegniamo lo script che contiene le parole chiave del vincolo di destinazione. Conserviamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel sito obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'istruibilità può generare viti di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini semantici, di completezza che di fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Dato che i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale consentire alle versioni più piccole e specializzate di possedere capacità di pianificazione linguistica. La creazione di dataset rappresenta un passaggio fondamentale per"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi precedenti non consentono di pianificare obiettivi specifici, e l'annotazione manuale dei dati richiede costi elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici di vita reale."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per la costruzione di un dataset di pianificazione del linguaggio vincolato denominato CodeScri."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo generato cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di validazione e di test. Chiediamo ai collaboratori di crowdsourcing di rivedere infine i redditi nei campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle limitazioni di CodeScript. Abbiamo riscontrato che Coscript manifesta un elevato pluralismo negli obiettivi specifici generati. Con Coscript, possiamo gestire modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con l'aumentare delle dimensioni, il punteggio t-five ottenuto dopo il fine-tuning può generare script di qualità variabile e, nei modelli di dimensioni maggiori, suggerire che modelli più piccoli possono sopprimere modelli più grandi, qualora adeguatamente addestrati su dataset appropriati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione linguistica vincolata. Abbiamo sviluppato un'abilità di pianificazione linguistica vincolata per i modelli linguistici di grandi dimensioni e abbiamo elaborato un metodo di filtraggio sovra generativo per i modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un dataset quadratico di alta qualità, Codecri, per la pianificazione linguistica vincolata. Speriamo che il dataset CodeSscript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. \nPer maggiori dettagli su Codecri, si veda il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Shu H.\nOggi presenterò il nostro articolo dal titolo \"Do Named Entity Taggers still work well in 2023?\", basato sul lavoro di Connell del 2003.\nIniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate, o il NER task."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano ConONO 2003 per sviluppare NER da quasi 20 anni, e ciò pone naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa capacità di generalizzazione, quali sono le cause del calo di performance di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset Connell++. Si tratta di un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e successivamente annotato secondo le stesse linee guida di annotazione di Connell del 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "quindi ottimizzate ulteriormente oltre 20 modelli su Conal 2003. Li abbiamo valutati sia sul set di test Con O3 che sul set di test Cono plus first."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la capacità di generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo riscontrato che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo aspetto riguarda l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli transformer tendono a generalizzare meglio a nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che, di norma, modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un task a valle. Qui, abbiamo anche riscontrato che un maggior numero di esempi di fine-tuning conduce effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "la nostra prossima domanda: cosa causa il calo di prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è l'overfitting adattivo, ovvero l'overfitting dovuto ai costi derivanti dal riutilizzo dello stesso set di test ripetutamente, e questo si manifesta solitamente come rendimenti decrescenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è il drift temporale, che consiste nel degrado delle prestazioni causato dall'incremento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting dativo, abbiamo osservato che, dal grafico a destra, la retta di migliore adattamento rossa presenta un gradiente maggiore di 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Colo 2003 si traduce in più di un'unità di miglioramento in Colo++, il che implica l'assenza di rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che l'overfitting adattivo in questo caso non si verifica."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E per quanto riguarda la temperatura?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per il fenomeno del drift temporale, abbiamo condotto un esperimento per riaddestrare o proseguire nel pre-addestramento di alcuni modelli con dati più recenti e abbiamo constatato che le prestazioni diminuiscono all'aumentare dell'intervallo temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo di performance sia lo scostamento temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di dimensioni maggiori del modello e di un numero maggiore di esempi di fine-tuning, e questi obiettivi si realizzano reciprocamente. Non possiamo avere un solo elemento, ma dobbiamo sviluppare tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo inoltre constatato che il calo di prestazioni riscontrato qui è dovuto alla deriva temporale e, in modo piuttosto sorprendente, non è causato dall'adattamento, nonostante l'utilizzo del metodo Connell del 2003 per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Tornando quindi alla domanda che abbiamo sollevato nel contesto del nostro articolo, i tagger Carnal 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un deciso sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare la generalizzazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, in caso di domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "ciao e parlerò del nostro lavoro sulla risoluzione di espressioni differenziali indirette per la selezione di entità, in cui presentiamo l'alt Entity Corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Philipp Radlinsky, Sylvia Parity e Annie Greece."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta e considerare questa alternativa domanda: intendevate \"Easy on Me\" o \"I Got a Feeling\"? Qui, un utente vuole selezionare una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto, ad esempio indicando il titolo della canzone, \"On Me\", o la sua posizione, il primo."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il titolo della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente desidera specificare una preferenza. Ecco alcuni esempi di differenze dirette, ad esempio il più recente o il segnale che non è energico."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "è un problema importante nei sistemi di conversazione e anche per il benchmark della comprensione delle entità da parte dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "non siamo a conoscenza di un dataset pubblico su larga scala per un compito specifico, pertanto ne raccogliamo uno utilizzando l'annotazione collettiva. Il nostro dataset copre tre domini differenti: musica, libri e ricevimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La metodologia di raccolta del data set enfatizza l'informalità attraverso l'utilizzo di un set di completamento a fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartone animato presenta tre fumetti. Nel primo fumetto, Bob dice: \"Ricorda quella canzone che stavamo ascoltando ieri?\" e con ciò Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "in questo secondo fumetto Alice dice:\nVuoi dire \"Easy on me\" o \"I got a feeling\"?"}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "è la quest alternativa e nel terzo fumetto Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio il nuovo amico."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "fornire automaticamente la prima e la seconda bolla di dialogo, ma la terza viene compilata dall'annotatore; la prima bolla di dialogo viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata nel modo seguente."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "si intende sempre utilizzare un modello semplice, ad esempio A o B, dove A e B sono estratti da Wikipedia?"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è solitamente più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "il primo è uniforme"}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità presentano titoli simili, ad esempio due libri con il nome \"the retail\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "La terza riguarda quando presentano descrizioni simili su Wikipedia e, infine, quando condividono informazioni, voci o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "mostriamo questa domanda alternativa agli studenti, i quali conoscono il nome di queste entità, ma non necessariamente conoscono l'entità stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "quindi, quello che facciamo è fornire alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link alla ricerca di Google per ciascuna canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "chiedete quindi agli annotatori di ascoltare almeno alcune parti di ogni canzone e di leggere informazioni su ciascuna canzone. Ecco, per esempio, il risultato della ricerca Google per la canzone \"easy answer\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo un testo di sfondo proveniente da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, anch'esse da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, chiediamo agli annotatori di scegliere una di queste entità, ad esempio, qui la prima, e descriverla utilizzando da tre a cinque espressioni referenziali indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi dal nostro set di dati: ad esempio, quello senza parole, non quello con il bambino di 12 anni, o quello fittizio, o quello proveniente dall'Azerbaigian e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus di alternative presenta 6.000 domande alternative distribuite su tre domini e include 42.000 espressioni referenziali indirette. I risultati ottenuti con il modello T5-X Large sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico avesse accesso allo stesso identico background di conoscenza degli annotatori, l'accuratezza sarebbe davvero elevata. Si attesterebbe tra il 92 e il 955%. Ma questa situazione non è realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, la precisione si attesta tra l'82 e l'87 percento, un valore più realistico, ad esempio quando il modello linguistico recupera tali conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "se il modello linguistico ha accesso unicamente ai nomi di entità, l'accuratezza è solo del 6 percento, quindi c'è ampio margine di miglioramento. Abbiamo inoltre dimostrato che i modelli sono generalizzabili a diversi domini. Ecco un link al nostro dataset. Grazie per."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sarah Papppy dell'Università di Trento e Foa scena Bruno Kessler e introdurrò brevemente l'articolo \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro congiunto con Matteo Negri e Marco Duchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simSD, è il processo di trascrizione in tempo reale del linguaggio parlato in testo in un'altra lingua, consentendo la comunicazione interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "quali sono i problemi dei modelli SimST attuali? Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complesse, ad esempio quelle che coinvolgono obiettivi di ottimizzazione differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E l'addestramento e la manutenzione di diversi modelli per raggiungere differenti regimi di latenza, ad esempio, addestrando un modello con una latenza media di un secondo e un altro con due secondi di latenza, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Allora, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Essere i primi a utilizzare modelli SD offline già esistenti senza riaddestramento o adottare architetture specifiche per SSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfrutta la conoscenza già acquisita dal modello attraverso il meccanismo di tensione tra l'input audio e l'output testuale, ovvero il meccanismo di crosstenzione, e potete osservare un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione consiste nel proporre un'attenzione decorale a punti o a codificatore, e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione si concentra."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, ovvero questa somma è al di sotto di una certa soglia alfa negli ultimi frame di discorso lambda, il che implica che l'informazione ricevuta è sufficientemente stabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede una traduzione in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E analizzeremo il peso dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i frame vocali ricevuti più antichi, mentre l'ultima parola fa riferimento agli ultimi frame vocali ricevuti come frame vocali lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "mentre, poiché la somma della tensione incrociata supera una certa soglia alfa, non emetteremo l'ultima parola e attendiamo un altro segmento discorsivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se proseguiamo e riceviamo un altro blocco di discorso e il nostro modello prevede più di tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola indica gli ultimi frame del discorso dell'Agnello."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole saranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osserva il risultato principale di un punto."}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tracciamo i risultati della traduzione simultanea delle pagine su grafici in cui abbiamo, da un lato, il blu che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "quella è la misura della latenza. e consideriamo anche la media consapevole della computazione, che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo quindi che le nostre cure si posizionino il più in alto possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma desideriamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie di plepara, anch'esse applicate a modelli offline, come la strategia withK e l'accordo locale. E confrontiamo inoltre con architetture all'avanguardia, specificamente progettate per la traduzione automatica della lingua simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea a velocità su testi tedeschi."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che un dubbio supera tutte le strategie applicate ai modelli offline, dato che le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo di calcolo impiegato, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo inoltre rilasciato codice, modelli e output simultanei ad open source, per agevolare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Ian e io e il mio collega Jion presenteremo la nostra ricerca su Multi-Instruct, un metodo per migliorare l'apprendimento sociale multimodale tramite l'affinamento delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli linguistici di grandi dimensioni, numerosi studi hanno iniziato a esplorare nuovi paradigmi di apprendimento che consistono nel riutilizzare modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente dal punto di vista dei parametri e dei dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di eseguire compiti inediti in modo efficace, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si è concentrata sul miglioramento delle prestazioni seriali in compiti basati esclusivamente sul linguaggio, trascurando la computer vision e i compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo investigare se l'istruzione di ottimizzazione su modelli proteintrain multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali inediti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una discrepanza considerevole nella disponibilità del dataset di addestramento tra RP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "esiste più di 1600 task di istruzione dedicati esclusivamente al pranzo; tuttavia, non esiste un dataset di instruction tuning multimodale disponibile su larga scala e pubblicamente accessibile, il che motiva la nostra iniziativa di creare un dataset di tuning multimodale per istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-Instruct, il primo dataset di benchmark per l'affinamento con istruzioni multimodali, composto da 62 diverse attività multimodali che coprono 10 ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "le attività derivano da 21 dataset open source esistenti e ciascuna attività è corredata da cinque istruzioni scritte dettagliate."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "L’indagine sull’affinamento dell’istruzione multimodale è il nostro dataset proposto. Prendiamo ofFA, un modello di training multimodale unificato, come modello di base. ofFA utilizza un vocabolario unificato per i token linguistici, le immagini e le coordinate di un riquadro di delimitazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo alcuni esempi tratti dal nostro dataset multi-istruzionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "unificare l'elaborazione di diversi tipi di dati di input e output."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato unificato sequence-to-sequence in cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ora parlerò di ottimizzazione dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il dataset di training, utilizziamo 53 task dal gruppo N per l'addestramento e campioniamo 10.000 istanze per task. Per il testing, riserviamo l'intero gruppo di comprensione del senso comune e selezioniamo cinque task aggiuntivi da WiQ e dal gruppo miscellaneous."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nella velocità di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla velocità di test dell'istruzione naturale come sullo stesso compito per NRP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, utilizziamo un modello OFA di grandi dimensioni pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata in modo casuale con uno dei suoi 5 modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Durante i test per ciascun compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando entrambe le 5 istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e il valore massimo delle prestazioni, unitamente alla deviazione standard delle prestazioni in tutti e 5 gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo rootjL. Per un compito RP, riportiamo anche RujL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto anche una metrica di valutazione aggiuntiva, denominata sensibilità. Questa misura la capacità del modello di produrre in modo coerente gli stessi risultati per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i nostri risultati principali. Come possiamo notare, l'affinamento tramite istruzioni può migliorare significativamente le prestazioni di OFE in compiti multimodali simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il trasferimento di apprendimento da un dataset di istruzioni naturali può giovare all'affinamento tramite istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, con l'aumentare del numero di task, il modello raggiunga prestazioni migliori e, contemporaneamente, una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto anche un esperimento. Abbiamo utilizzato un'unica istruzione rispetto a cinque istruzioni. Come possiamo osservare, l'utilizzo di più istruzioni può migliorare significativamente le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra l'effetto di differenti strategie di ottimizzazione frontale sulla sensibilità del modello. Come possiamo notare attraverso il transfer learning da un dataset di istruzioni naturali, il modello può raggiungere una sensibilità significativamente superiore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Si può anche osservare come il transfer learning dai dati di istruzione Nitro possa aiutare OFA a raggiungere prestazioni significativamente migliori sul data set di istruzioni NitroE."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo proposto il primo dataset di ottimizzazione dell'istruzione multimodale su larga scala. WithFA continua a migliorare le capacità neurali di OFA e stiamo esplorando diverse tecniche di apprendimento per trasferimento, dimostrando che vi sono dei vantaggi. Abbiamo progettato una nuova metrica denominata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, un'altra cosa: stiamo raccogliendo set di dati di ottimizzazione tramite istruzioni multimodali molto più ampi, con circa 150 ulteriori compiti di variazione linguistica, e li rilasceremo. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koovsna, e sono lieto di darvi il benvenuto alla presentazione del nostro articolo per ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Baqui, Aaron Muller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, riprendiamo il paradigma delle coppie minimali."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il minimal pairtopara valuta essenzialmente i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la grammaticalità – come in esempi del tipo \"blimp\" – o l'accettabilità in termini di stereotipi, come nelle coppie di stimoli \"crowds\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma della coppia minima, il modo tipico per valutare i modelli linguistici è presentare una frase accettabile o grammaticalmente corretta e poi presentare una frase inaccettabile o agrammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E allora si spera che il modello attribuisca, fondamentalmente, una maggiore probabilità al compromesso accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non ci consente sostanzialmente di valutare l'accettazione dei modelli nei confronti di frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggigiorno, i modelli linguistici di grandi dimensioni stanno generando contesti sempre più ampi. Pertanto, è fondamentale valutare l'accettabilità del modello durante l'intero intervallo di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "Ed è questo che stiamo cercando di fare qui. Stiamo cercando di riesaminare la pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questo è l'approccio. Ciò che facciamo, per simulare queste sequenze più lunghe, è rianalizzare i dataset stessi e quindi ricreare frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da tali dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto una coppia tipica di amticità dal set di dati BbliIM, relativa al caso dell'isola aggiuntiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe che siano accettabili e che presentino la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticali da un corpus pilota."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare lo stesso scegliendo frasi inaccettabili provenienti dallo stesso set di corrispondenze, e ciò potrebbe essere utilizzato, a sua volta, per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo farlo anche scegliendo frasi da un sottoinsieme diverso o da un set di dati differente. Questo è ciò che definiamo scenario di disallineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati che state utilizzando per la valutazione. Possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci dirà, per esempio, se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Come se il contesto provenga da un sottoinsieme differente del dataset oppure se sia completamente irrilevante rispetto alla frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "E dunque, come si comporta il modello? Innanzitutto, consideriamo le frasi di Wikipedia che sono completamente irrilevanti per la coppia di query corrente, e lì riscontriamo che i giudizi MPP sono per lo più robusti per una lunghezza del contesto arbitraria."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 2024 token per sfruttare al massimo i modelli OPT e GPT2. E come si può osservare qui, nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "E ora, cosa succede quando scegliamo frasi dallo stesso set di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Ed ecco che ci troviamo a scegliere o creare frasi da domini accettabili e non accettabili, provenienti dallo stesso set di dati della palestra sintattica BlimIM gymIM."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando facciamo corrispondere la struttura, cioè quando selezioniamo le frasi provenienti dallo stesso fenomeno nel paradigma di tassazione della persona responsabile,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora, questo — e questo è molto rilevante, come questo effetto aumenta con la lunghezza del contesto — e ciò influenzerebbe probabilmente i modelli linguistici più recenti che dispongono di una finestra di contesto ampia."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso della corrispondenza influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, tentando di preservare la struttura rilevante ma introducendo del rumore. E dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che nessuno di questi rumori sta effettivamente inducendo il modello a modificare il suo corso in termini di come ci mostra l'andamento del giudizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "Fondamentalmente, riscontriamo che i modelli sono sensibili al periodo delle frasi in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "È in quel momento che perturbiamo le frasi all'interno del dominio accettabile, che osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi all'interno del dominio di approvazione accettabile, notiamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i principali risultati del nostro lavoro dimostrano che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, come la conduciamo attualmente con input brevi e in singole frasi, potrebbe non catturare appieno la conoscenza astratta dei modelli linguistici all'interno della finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Vi preghiamo di leggere il nostro articolo per maggiori dettagli sugli esperimenti condotti.\nGrazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Just John, dell'Università della Pennsylvania (Penn State University). Oggi presenterò il nostro lavoro, Exemplar: analisi semantica cross-linguale in molteplici lingue naturali e rappresentazioni manuali."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "l'elaborazione semantica è un compito volto a costruire rappresentazioni semantiche di query utente quali ZQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "La traduzione semantica cross-linguale è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "come illustrato nella figura, è necessario tradurre la query in diverse lingue naturali utilizzando modelli neurali in SQL, Lambda, funQL e simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "Esistono modelli di parsing semantico cross-linguale proposti e valutati separatamente su un insieme limitato di esempi e applicazioni. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono lacune nella copertura di alcuni aspetti del linguaggio naturale che il cinese non considera, e."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Lacune di copertura in determinate molteplici rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "vengono valutati solo su determinati modelli neurali; per esempio, è previsto un solo modello per la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, abbiamo proposto Ex exampler, ma forniamo un dataset exampler uniforme per il semiperssaggio incrociato in molteplici lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "contiene 90 insiemi nei domini virali, 5 elementi seman in tassonomia, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l’addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione: useremo l'API di Google Translate per tradurre il testo di partenza nella lingua di destinazione, per poi utilizzare un modello monlinguistico per addestrare eventuali valutazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E per esempio, addestriamo il modello inglese su query in lingua inglese e, durante l'inferenza, traduciamo la query in tedesco tramite API in inglese, per poi utilizzare il modello addestrato per prevedere l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "testeremo anche il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "anche testare l'impostazione monolingue futura allenando modelli monolingui con solo il 10 percento dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E quale è la modellazione di un modello multilingue, che addestriamo con un unico modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, raggruppiamo le query in tedesco, inglese e cinese per addestrare un modello multilingue e, durante l'inferenza, possiamo utilizzare anche questo modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "Per tradurre query in tedesco o query in cinese o simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche il trasferimento a freddo interlinguistico e il trasferimento a freddo. Addestriamo su una lingua di origine e trasferiamo a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'addestramento, lo alleniamo su query in inglese o su una combinazione di query brevi in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "troviamo anche molti risultati interessanti. Pertanto, per quanto riguarda l'analisi dei modelli monolingue, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "inclusi encoderPDdR, acronimo di multilingual pretrained encoders with pointer-based decoders, quali X elementr plus pdr e bird plus pdr"}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo anche i modelli encoder-decoder, inclusi i modelli encoder-decoder pre-addestrati multilingue come BERT e Mt5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "ha rilevato che l'architettura encoder-decoder ottiene le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "valutiamo il nostro Mmt5 e l'esempio xlmr plusPDdr nelle nostre configurazioni multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "questo, encoder-decoder o encoder PDR può essere migliorato tramite l'addestramento in una miscela di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che ciò è dovuto al fatto che la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni dell'inglese diminuiscono in sette dataset e ottengono un miglioramento solo in tre dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come i Curdi della multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre il divario di performance interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento Fu cross-linguale; la linea arancione rappresenta il trasferimento zero-shot cross-linguale, mentre la linea verde rappresenta la configurazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che confrontando la linea verde e quella arancione, abbiamo rilevato che per l'impostazione zero di lunghezza lo scarto nelle prestazioni del trasferimento interlinguistico è significativo e confrontando la linea blu e quella arancione, abbiamo riscontrato che per poche impostazioni di lunghezza lo scarto di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato anche altri risultati interessanti. Ad esempio, l'architettura encoder-decoder supera i modelli proW o ha ottenuto risultati comparabili. L'affidamento all'inglese come lingua naturale può incrementare significativamente le prestazioni dei futuri modelli su lingue naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che modelli linguistici multilingue come coders e blue risultano ancora inadeguati per classi di semi-supervisione cross-linguale."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo costruito un benchmark unificato per l'analisi semantica cross-angle con molteplici lingue naturali e diverse rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "effettuare uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingue e i nostri risultati mostrano numerosi risultati interessanti e così via. Vi invitiamo a visitare il nostro articolo e il codice sorgente. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Tutti, io sono Al Villaad e fornirò anche una breve panoramica del documento \"Printing Palm: Translation Assessing Strategies and Performance\". Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È stato addestrato su una vasta raccolta di testi comprendente 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "duma per la cucina raggiunge lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo un primo studio sistematico del prompting per modelli linguistici di grandi dimensioni applicato alla traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò implica l'impiego degli insiemi di test più recenti per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "confrontiamo con sistemi all'avanguardia, quali i sistemi con le migliori prestazioni o le valutazioni WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche neuralMT all'avanguardia e presentiamo inoltre i risultati di una valutazione umana basata sull'esperienza di esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "il prompting ha una notevole influenza sulle prestazioni dei sistemi di traduzione automatica neurale (lnms), come si evince da un semplice esperimento in cui utilizziamo un prompting breve e forniamo due prompt differenti per frasi diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "nella maggior parte delle frasi, 516 su 1000, la differenza osservata è superiore a un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "In casi estremi, ciò può arrivare fino a 40 punti di sfocatura. Pertanto, è importante selezionare una strategia di prompting efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "i nostri esperimenti con una strategia di prompting a cinque colpi, in cui semplicemente contrassegniamo le frasi che forniamo al sistema con la lingua in cui sono espresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, che rappresentano le frasi di partenza, sono contrassegnate con i due punti tedeschi e le traduzioni inglesi con i due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "ho notato che la forma effettiva della stampa non ha una grande influenza nel caso di diverse stampe brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e un colpo, e quando si passa, come nel nostro caso, al prompting a colpi fattuali, la differenza rispetto alla forma effettiva del prompting è praticamente trascurabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "gli esempi che portano il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo le richieste di selezione provenienti dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono stati creati in quantità molto maggiore e con una qualità superiore rispetto ai dati di training, che sono più semplici e portano a prestazioni migliori quando si utilizzano i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "ciononostante, sistemi specializzati all’avanguardia presentano un vantaggio notevole rispetto alle traduzioni pan, ma uno si avvicina piuttosto a un sistema commerciale; nel nostro caso, abbiamo scelto di evitarlo, optando per Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che otteniamo dall’emailazione che svolgiamo utilizzando il framework MQN è che la fluidità di Palm è comparabile a sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra dunque che Palm scelga talvolta di produrre una traduzione dal suono migliore omettendo parti della frase originale che sarebbero state trasmesse nelle traduzioni letterali."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di stile esterno per la padella è inferiore rispetto a quella dei sistemi all'avanguardia, il che costituisce un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che parm fornisce un output davvero fluido, ma con alcune problematiche di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "Ed ecco tutto per questa brevissima panoramica. Per maggiori dettagli, vi invito a consultare la presentazione completa del paper. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Dawei, dottorando presso l’Università di Silent in Germania. In questo video vorrei presentare il nostro recente lavoro: “Biggerer Than You Think”, uno sguardo critico al settimanale Surprise Lening."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "è un lavoro congiunto con Sha My Muba, Gear Stefan e Ditishklakov."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "weak supervision\n\nnon etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o estrazione di codice di località, come illustrato nella figura e a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "rispetto alle annotazioni umane, le annotazioni di qualità inferiore sono notevolmente più economiche, ma sono anche più rumorose, il che significa che una certa percentuale di esse risulta inaccurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "addestriamo direttamente reti neurali su dati con etichette deboli, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Vengono proposti algoritmi di training per l'apprendimento con supervisione debole per addestrare robustamente reti neurali in presenza di rumore nelle etichette, in modo che i modelli addestrati mantengano una buona capacità di generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "recenti lavori in wSL, dove wSL sta per \"weekly support learning\", sostengono comunemente che i modelli vengono addestrati esclusivamente sui dati delle etichette settimanali per ottenere prestazioni elevate su insiemi di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, l'affermazione non è errata, ma sussiste una riserva."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "è che spesso si presuppone l'esistenza di un set di validazione aggiuntivo o una selezione del modello tramite un approccio well-formed."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "ci si è bloccati su questa impostazione del problema, ma ciò implica che siano necessarie annotazioni manuali aggiuntive nell'apprendimento di supporto settimanale; tuttavia, come un elefante nella stanza, questa necessità è spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo adottato tale approccio per porre tre domande di ricerca. Innanzitutto, i dati di validazione puliti sono necessari per WSL? o possiamo forse utilizzare un set di validazione rumoroso al loro posto?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "se è necessario disporre di dati puliti, o se la pulizia dei dati è obbligatoria affinché WSL funzioni, quanti dati puliti ci servono, infine? Dovremmo utilizzare esclusivamente i campioni puliti per la convalida, oppure esistono metodi migliori per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, scopriamo che, in modo interessante, i recenti metodi WSL richiedono effettivamente campioni di wideation puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti si verifica un significativo calo delle prestazioni. Come illustrato in questa figura, in assenza di campioni di validazione puliti, i modelli di tendenza non riescono a generalizzare al di là delle etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "quello della formazione è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "indica che gli approcci WsSL richiedono effettivamente dati pulitamente etichettati per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo risultato è che l'aumento del numero di campioni di validazione puliti contribuirà a migliorare le prestazioni degli approcci WSL, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "ci occorrono solo 20 campioni per classe per ottenere prestazioni elevate."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se decidiamo in ogni caso di accedere a campioni puliti, l'addestramento diretto su questi ultimi raggiungerà persino prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di performance tra approcci di fine-tuning applicati direttamente sui dati puliti e approcci WSL che utilizzano i dati puliti unicamente per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "possiamo constatare che, se disponiamo di 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "possiamo constatare dai dati che il modello valido, denominato ftw, inizialmente sottoperforma rispetto a metodi WSL più complessi, come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": ", se consentiamo a fantuni di continuare sui campioni puliti, allora Tw ottiene risultati paragonabili a quelli di altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "In pratica, non vi è motivo di scegliere metodi WSL più complessi che richiedono maggiore tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "riassumendo, abbiamo dimostrato che gli approcci wSL recenti richiedono campioni annotati manualmente e privi di errori per funzionare correttamente; i loro guadagni prestazionali e la loro praticità sono ampiamente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Di seguito, alcune raccomandazioni concrete per le future disposizioni sull'orario di lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, indicare i criteri di selezione del modello. Ad esempio, specificare se la sezione del modello è stata realizzata utilizzando campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con pochi e brevi benchmark di atterraggio, piuttosto che con lavori su campioni di calcestruzzo.\nIn terzo luogo, il fine-tuning continuo è un benchmark semplice ma efficace che dovrebbe essere preso in considerazione in futuri lavori in WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo reso pubblico il nostro codice sorgente. Potete trovarlo tramite il codice QR presente in questa diapositiva. Vi invitiamo a consultarlo liberamente. Grazie e buona conferenza a tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono James Finch\ne io sono Sarah Finch.\nE oggi vi spiegheremo tutto su ABC Eval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dall'Emory NLP Lab, guidato dal Professor Gino Choi presso la Emory University, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che tu abbia appena sviluppato un modello di dialogo e desideri valutare quanto bene si confronti con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale tra due conversazioni sia migliore o di valutare le conversazioni su una scala liquoradica."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo presenta molteplici aspetti. Pertanto, potrebbe essere utile valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "l'approccio consiste nel chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o di scala liquor esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "questo approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprima determinati comportamenti, come fornire informazioni irrilevanti o contraddire se stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Definiamo questo approccio come l'annotazione dei comportamenti in chat, o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo la letteratura recente, sono stati ritenuti influenti sulla qualità della chat."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare i tassi con cui i modelli di chat commettono diversi errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "ABCEval misura il numero di turni in cui un modello conversazionale ignora il proprio interlocutore o fornisce una risposta irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o il suo partner allucina fatti errati o viola la conoscenza del senso comune e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione fosse più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi esistenti: valutazioni della qualità a livello di turno, valutazioni della qualità a livello di dialogo e confronti a coppie a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la prassi standard per la valutazione dei modelli di dialogo su molteplici dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi di questi risultati di valutazione, abbiamo riscontrato che le etichette comportamentali ABC sono complessivamente più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in modo doppio."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABCEval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostra questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, si può osservare come la misurazione della proporzione di turni con contraddizioni proprie e con il partner spieghi rispettivamente il cinque e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza del discorso ne spieghino solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ciascuna metrica di valutazione catturi un aspetto unico della qualità della chat utilizzando una regressione lineare a gradini."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può capire come la combinazione di tutte le metriche ABC Eval spieghi oltre il 25% della qualità della conversazione. E rimuovendo le metriche una per una, la maggior parte di esse comporta la perdita di una quantità considerevole di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D’altro canto, la combinazione di tutte le metriche relative alla fase di svolta (turn-level) spiega notevolmente meno la qualità, e un numero inferiore di queste metriche contiene informazioni univoche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Metriche ABC Eval affidabili, informative e distintive ci consentono di valutare l’intelligenza artificiale conversazionale con una maggiore risoluzione rispetto a quella raggiungibile con metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare, dai risultati del nostro esperimento, che diverse sfide rimangono ancora irrisolte e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o contrastano con il proprio interlocutore nel 10% dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "data il rapido ritmo di miglioramento nel settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dalla nostra valutazione; tuttavia, ciò è tanto più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "auspichiamo che ABC Eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione, e attendiamo con interesse di vedere come l'intelligenza artificiale conversazionale si evolverà nei prossimi mesi e anni. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "salve, mi chiamo Kyyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto: un'esplorazione multilingue basata sui dati\". Questo lavoro è stato svolto in collaborazione con Patrick Fernage, Emiliu Andre, FD Martins e Graham Newbigin."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo “mole” in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "se la frase precedente dovesse iniziare a diventare pericolosa qualora i ministri lo scoprissero, allora \"more\" si riferisce a una spia. Ma se la frase precedente fossePotrebbe essere qualcosa di serio, dottore? allora \"more\" si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, ne cambia anche la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli riescano a distinguere casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola porzione di traduzioni dipende dal contesto, il che rende metriche a livello di corpus come BLEU incapaci di catturare tali traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipologie limitate di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché in genere si basano sulla conoscenza del dominio e sulla curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, ci proponiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando in che misura il lavoro di traduzione dipenda dal contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E ciò viene fatto misurando quanta informazione il contesto C fornisce riguardo al target Y, dato il source X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "si può considerare il CXMI come l'informazione acquisita fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l’utilizzo del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un elevato PA6MI come quelle che necessitano di contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto piecexMI per individuare schemi tra di esse."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi su trascrizioni di TED Talks tradotte dall'inglese in 14 diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre diversi livelli. In primo luogo, esaminiamo i tag grammaticali che presentano elevati valori medi di pxMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "questo ci permette di individuare, ad esempio, pronomi duali in arabo che presentano un p6MI relativamente elevato. Questo può essere spiegato dal fatto che l'inglese non possiede pronomi duali, pertanto è necessario il contesto per determinare se un pronome sia duale durante la traduzione in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, riscontriamo che anche certe lingue richiedono un contesto quando si vuole scegliere la forma verbale appropriata. Esaminiamo quindi gli elementi lessicali che presentano un’alta pxMI calcolata su tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello che vediamo qui, in cui in cinese è necessario il contesto per tradurre i nomi propri, al fine di garantire che si utilizzi la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, riscontriamo che il contesto è supportato per mantenerlo nella formalità appropriata."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "finalmente, esaminiamo diversi token individuali che presentano un alto valore di p6MI. Ciò ci permette di identificare fenomeni che non possono essere veramente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ora utilizziamo i risultati derivanti dalla nostra analisi per progettare un benchmark per la traduzione innovativa di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che vi si riferiscono, e chiamiamo il nostro tagger \"multilingua consapevole del discorso\" o tagger muda."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche notare che le diverse lingue presentano proporzioni differenti di questi fenomeni discrettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "quindi si utilizza l'etichettatore M applicando l'etichettatore sul corpus parallelo che desideriamo utilizzare per la valutazione e si applicano le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto identificati dall'etichettatore M."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "E infine, utilizziamo il nostro benchmark, unitamente ad altre metriche, per valutare diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, ad esempio per la valutazione di BLEU, riscontriamo che i modelli agnostici di Conic ottengono le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "allora, se utilizziamo commenti, i modelli context-aware ottengono i risultati migliori. E se utilizziamo la misura wordf, allora i modelli con o senza contesto presentano prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il sistema di traduzione a livello di documento ottimale qualora si utilizzino esclusivamente metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "utilizziamo il benchmark MUDA per valutare i modelli e constatiamo che i modelli sensibili al contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi modelli non sono significativamente migliori dei modelli che non sfruttano il contesto in altri fenomeni, come l’ellissi, i pronomi e le forme verbali. Questo suggerisce, quindi, dove sarebbe necessario vedere un maggiore progresso per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo inoltre confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeP è generalmente più preciso di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere, conduciamo un'analisi basata sui dati su 14 coppie linguistiche per individuare i casi in cui le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri filtri per costruire un punto di riferimento per la traduzione automatica a livello di documento, il quale può aiutarci a identificare quali modelli di fenomeni del discorso discorsivo sono in grado di gestire bene o meno e quali sistemi di traduzione sono validi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio vivamente per la Sua attenzione. Ci vediamo su Trado."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanislavak e vi presenterò i nostri lavori su Dr. Bert, un modello preaddestrato robusto in francese per i domini biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, inizieremo discutendo della modellazione del linguaggio in Herke. Successivamente, presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in lingua francese, denominato Dr. Bert, basato su Roberta e addestrato su Naos, un dataset di dati medici estratti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo inoltre un confronto di modelli con molteplici impostazioni protoniche e fonti di dati. Successivamente, illustriamo i nostri risultati su 11 compiti a valle biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine concludiamo con le sperimentazioni e forniamo ulteriori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dalla sua pubblicazione nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un notevole incremento delle prestazioni rispetto a metodi storici, statici e contestualizzati come word2vec, fastText o GloVe."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come in francese, con Cammbert, e ad altri ambiti, come il biomedico, con Permed Bert e Biobert, e poi in ambito clinico-ostetrico, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della scarsità di dati nel dominio specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, la Francia non disponeva di alcun modello open source per la biomeliconica."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci poniamo quindi una domanda: quali sono le fonti di dati più appropriate per un'ampia gamma di applicazioni, e tali dati grezzi possono essere una valida sostituzione dei dati clinici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo confrontato il Dr. Bert con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non specializzato della nostra struttura."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo: quanta quantità di dati è necessaria per addestrare un modello specializzato su dati francesi? Quattro gigabyte, un gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo inizialmente quattro modelli sviluppati autonomamente: una prima versione di D. Bert con sette gigabyte di dati di allenamento, una seconda versione con quattro gigabyte di un set di dati di allenamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che costituisce un modello clinico, con quattro gigabyte di frasi estratte da nodi clinici, e una versione finale di Schubert, con un mix di quattro gigabyte di insiemi di dati naturali e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati tramite contra pre-training per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso di Cammbert e addestrato su quattro gigabyte di insiemi di nachls; un altro anch'esso basato su Cammbert, ma addestrato questa volta su quattro gigabyte di nodi Kcliner."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, un modello basato sul biomedicale inglese, Bermed Bert, addestrato su quattro gigabyte di estratti. In totale, disponiamo di sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo diverse attività a valle pubbliche e private, quali il riconoscimento di nomi ed entità, la classificazione, l’etichettatura grammaticale e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di tipo B, ovvero Cammbert Oscar da 18 gigabyte, Cammbert Oscar da quattro gigabyte, Cammbert cinet da quattro gigabyte, Lomet Bert, Biobert e Clin BERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione delle evidenziazioni, quel modello che performa meglio nel compito con dati di natura simile a quelli su cui il modello è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere quei dati e possiamo osservare che i dati provenienti da fonti eterogenee risultano più versatili. Osserviamo inoltre che l'utilizzo di una maggiore quantità di dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In definitiva, la formazione gratuita da zero sembra ottenere prestazioni superiori nella maggior parte delle attività."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul controllo del fingimento, utilizzando il peso e il tokenizer di permit Bir addestrato sul sottoinsieme di quattro Gigabyte di dati naturali, ha mostrato risultati comparabili a quelli ottenuti con Dr. Bert quattro Gigabyte addestrato da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Il che non è il caso del modello basato sui tokenizzatori e sui bianchi di Cammbert, che presentano problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la nostra architettura ottimizzata raggiunge prestazioni superiori in nove delle 11 attività successive, superando a livello globale i risultati del modello generico qui considerato, CamemBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specialistici sono preferibili, dati ancora più specialistici sono preferibili, ma non scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "tutti i modelli pre-addestrati ottenuti da nachos sono liberamente disponibili e consultabili; inoltre, tutti gli script di addestramento sono presenti nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "quindi grazie per questa presentazione e attendiamo con interesse le azioni che verranno intraprese durante la sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il multiset tagging e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "È frutto di una collaborazione con i miei relatori, Alexander Kola e Ivan Tittov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "la generalizzazione composizionale può essere compresa come la capacità di un discente di gestire una ricorsione più profonda e combinazioni inedite di frasi che sono state viste singolarmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione composizionale potrebbe apparire così. Come di consueto, disponiamo di un insieme di addestramento di enunciati. In questo caso, la ragazza dormì, e Maria sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste espressioni sono associate a forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard di machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente inedite."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha riscontrato una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "i modelli sequence-to-sequence ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, essi spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, quali quelle evidenziate a colori negli esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo più diffuso per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono concepiti per catturare il processo compositivo che mette in relazione le espressioni con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta costoso dal punto di vista computazionale. Tipicamente, ciò comporta una considerevole pre-elaborazione specifica per il formalismo delle forme logiche, ad esempio per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può anche implicare procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello neurale sequence-to-sequence che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte capacità di generalizzazione a ricorsioni più profonde senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "l'approccio prevede l'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che compariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nella seconda fase, utilizziamo un altro modello per prevedere una permutazione che ne definisca l'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un nuovo metodo per predire una permutazione che non impone vincoli rigidi sulle permutazioni possibili. Ciò rende il nostro approccio particolarmente flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo da sinistra a destra attraverso l'output e determiniamo quale token di multinsieme inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, passiamo al successivo token multi-insieme per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile, saltando a un altro token del multinsieme. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Finché ogni token della prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anticipazione dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sul benchmark COGs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcune altre forme di generalizzazione strutturale rimangono, tuttavia, estremamente complesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-setter provenga, il che pone una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono multiple permutazioni coerenti con i dati, ma quella linguisticamente corretta risulta latente. Abbiamo affrontato questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che l'individuazione della permutazione con il punteggio più alto è Np-difficile. Questo perché è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Lo approssimiamo con una rilassamento continuo, compatibile con GPU, che ci consente anche di propagare all'indietro attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate approfondire le nostre sperimentazioni e come affrontiamo queste sfide, vi invitiamo a consultare il nostro articolo scientifico o a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Akshata e oggi, insieme al mio coautore Martin, presenteremo il nostro lavoro, Kit Master: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple. Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio attingono a diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite un pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "i lavori in compiti come la risposta a domande dimostrano che i modelli possono utilizzare conoscenze temporali pre-addestrate per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "John ha visto il nuovo presidente eletto in TV.\n\nNow, please provide the English text you would like me to translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono conoscere in modo affidabile chi sia questa entità specifica, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato rispetto al periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, modelli efficaci per attività di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia conoscenza pre-addestrata che conoscenza acquisita durante l’inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "presentare un compito fondamentale di risoluzione delle referenze, progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti a uno studio umano e definiamo modelli di risoluzione delle referenze fondamentali."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Servin è un giudice. Kia è una fornaia. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro trascorsa a decidere casi in base a un codice legale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui il pronome \"he\" si riferisce, che in questo caso è il sermone."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome richiede due tipologie di informazioni: in primo luogo, conoscenza specifica dell’entità, come il fatto che “servile” è un giudice, e in secondo luogo, conoscenza generale, come il fatto che i giudici decidono casi in tribunale."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica per tipo di entità viene tipicamente osservata in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "variate la disponibilità di queste due informazioni in modo che possano essere reperite sia in un'unica fonte che in diverse fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo definito tre impostazioni di Kitmos: la prima con l’impostazione tipica di background pre-training, dove la conoscenza pregressa è considerata disponibile durante l'addestramento libero."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "secondo, c'è il contesto, sia l'ambiente dove la conoscenza pregressa è disponibile, sia durante la fase di pre-addestramento che durante l'inferenza. Infine, c'è il caso dell'ambiente esperienziale con entrambi i tipi di conoscenza disponibili unicamente durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "L'ultimo scenario è particolarmente interessante in quanto simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove professioni si sono sviluppate dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "è un esempio di come controlliamo la disponibilità dei fatti nelle due fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto pre-addestrato che assumiamo, la conoscenza di base che i politici ricercano per ottenere un seggio eletto nel governo è contenuta nei parametri pre-addestrati. Nel contesto interferencetime, forniamo la conoscenza anti-specifica: chechester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "nel contesto della scheda di interferenza, forniamo non solo conoscenze anti-specifiche, ma anche informazioni di base sui politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "Il contesto libero che forniamo presenta il tour fittizio basato sul merito, anziché la figura del politico, poiché un tour basato sul merito sarebbe difficilmente contenuto nella regione pre-T20peri."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "valutare il dataset sia con partecipanti umani che stabilire modelli di risoluzione delle preferenze. In questa figura mostriamo i risultati dei modelli con le prestazioni migliori sulla variante più complessa del contesto pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "il nostro addestramento specifico su Kidmus, entrambi i modelli non ottengono buoni risultati. Quando addestrati su Kidmus, tuttavia, sia C2F che il modello costruito per QF performano significativamente meglio rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che, quando addestrati su dataset generici di risoluzione di riferimenti, i modelli imparano a sfruttare indizi superficiali che non sono utili quando testati su Kidmus, dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "esperimenti aggiuntivi in cui la conoscenza fittizia ha indicato che persino i modelli con le migliori prestazioni non riescono ad integrare in modo affidabile la conoscenza retroattiva, unicamente durante la fase di interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "riassumere i principali risultati del nostro articolo: molti modelli di evoluzione della coreferenza sembrano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Ciononostante, anche i modelli più performanti sembrano avere difficoltà a integrare in modo affidabile conoscenze pregresse presentate unicamente al momento dell'inferenza. Se desiderate maggiori dettagli, consultate il nostro articolo e verificate il set di dati nel codice su github. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo sulle \"personas\" etichettate, impiegando prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato svolto in collaborazione con Essenndermush e Danjorovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi nei modelli linguistici di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Queste misure presentano diverse limitazioni, in quanto si basano abitualmente su set di dati costruiti manualmente, la cui cura risulta estremamente dispendiosa in termini di tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "misurano, inoltre, di solito solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre fasce demografiche o contesti, oppure catturano semplici associazioni molto generali e ampie, come associazioni negative con particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali complesse possano amplificare i pregiudizi e costituire specifici focolai di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "per superare queste limitazioni ci affidiamo alla proprietà per cui questi più recenti modelli linguistici ottimizzati per le istruzioni sono molto abili nel rispondere a istruzioni e prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, possiamo chiedere al modello di generare una persona, ovvero una rappresentazione di un individuo immaginario, utilizzando un prompt per, ad esempio, immaginare di essere una donna asiatica. Descrivetevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "possiamo immediatamente notare che questo è altamente generalizzabile a qualsiasi gruppo demografico, poiché possiamo semplicemente specificare qualsiasi marcatore identitario desideriamo all'interno di questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni da GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente notiamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di questi termini,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "Una donna asiatica è descritta come modesta. La donna mediorientale è definita con termini come esotico e con riferimenti a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le figure femminili di colore fanno riferimento alla loro ascendenza, mentre la figura maschile bianca non presenta nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "catturare questi schemi, il nostro metodo si articola in due fasi. La prima consiste nella generazione di queste personas."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste personalità sono stati ispirati da uno studio in cui tali suggerimenti venivano forniti a soggetti umani, scoprendo che, sottoponendoli a soggetti umani, era possibile anche far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "questo consente inoltre un confronto diretto tra le nostre personas generate e le risposte scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "la seconda parte è costituita dalle parole distintive, un metodo per identificare le parole che distinguono i gruppi distintivi da quelli che noi consideriamo tali, cosa che spiegherò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi molto specifici senza doverci basare su un lessico particolare."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, il metodo delle parole segnalate si basa sul concetto sociolinguistico di marcatività, che stabilisce l'esistenza di una forma predefinita e non marcata, e che qualsiasi gruppo che si discosta da tale forma risulta linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, la parola uomo, o scusate, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, specificano di solito un guerriero uomo e contrassegnano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono di solito marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, designiamo innanzitutto quali sono i gruppi non marcati e marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "dopodiché confrontiamo le persone utilizzando il metodo delle parole chiave, che consiste essenzialmente nell'impiegare log-odds ratio ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le persone che rappresentano donne nere, useremmo termini di sfida e confronteremmo i rapporti degli dei-legge con sia le persone bianche che le persone maschili, poiché questi sono i due gruppi corrispondenti non marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ed ora, alcuni risultati. Innanzitutto, utilizziamo il lessico degli stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": ", quando osserviamo effettivamente la distribuzione delle parole nel lessico, riscontriamo elementi molto diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene le persone generate presentino frequenze significativamente più elevate delle parole Luxon, quelle scritte dagli esseri umani mostrano una distribuzione di parole molto più ampia, mentre le parole stereotipiche presenti nelle persone generate sono in realtà limitate a \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, soltanto quelli positivi o, quantomeno, non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "Ed in effetti, il lessico non riesce proprio a cogliere molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole marcate per dimostrare come queste parole apparentemente positive favoriscano stereotipi e narrazioni essenzialistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi marcati, le parole più frequenti includono concetti quali cultura, tradizione, orgoglio ed esotico. E queste parole definiscono tali gruppi unicamente in relazione alla loro identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "contribuisce a una lunga eredità di discriminazione e di esclusione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si riscontrano numerosi tropi ricorrenti che si riflettono in queste parole, in particolare per le donne di colore. Ad esempio, le parole che descrivono una donna latina includono termini quali \"vivace\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "Ehm, che si collega a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come \"petit\", \"delicata\" e \"setosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "si ricollega a una lunga storia di iper-sessualizzazione delle donne asiatiche, considerate estremamente docili e sottomesse e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "finalmente, per le donne di colore, osserviamo che alcune delle parole più frequenti sono termini come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "si collega a un archetipo che è stato definito l'archetipo della \"strong black woman\", e sebbene a prima vista possa sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono state presentate ricerche che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché esercita una notevole pressione su tali gruppi demografici affinché siano resilienti e forti di fronte alle difficoltà sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "anziché impegnarsi concretamente a rimuovere tali ostacoli, esercita pressione su tali individui affinché li superino, il che conduce a esiti sanitari decisamente negativi per queste persone, tra le altre conseguenze dannose."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "in linea generale, constatiamo che i termini per ciascun gruppo segnalato riflettono sostanzialmente narrazioni fortemente essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché molti aspetti potrebbero essere trascurati se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, ci sarebbe davvero bisogno di una maggiore trasparenza riguardo ai metodi di mitigazione dei bias."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a qualche sorta di… strano."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un'eccessiva ed eccessiva allineamento dei valori in atto, oppure forse qualche altro metodo, ad esempio antistereotipico, che sta generando questi dannosi schemi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "proprio non si possono fare assunzioni né studiare ulteriormente la questione senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio moltissimo per aver ascoltato, ehm, si diverta all'Ace."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jing Wei Y e sono della University of Science and Technology of China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È con piacere che vi presento un breve video promozionale del nostro articolo. State replicando il mio modello, tutelando il diritto d'autore dei modelli linguistici di grandi dimensioni per embedding e servizi? Supportate il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo innanzitutto il contesto relativo ai servizi di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i modelli linguistici di grandi dimensioni come GPT, Lama e PM eccellono nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi costruiti sulle basi di modelli linguistici di grandi dimensioni per supportare diverse attività di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "OpenI offre un'API di embedding basata su modelli aGbt."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi recenti hanno dimostrato che l’attaccante può sottrarre il modello attraverso l’apprendimento basato sugli embedding e fornire servizi simili; pertanto, è necessario tutelare il diritto d'autore degli embedding intesi come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per tutelare il diritto d'autore dei servizi di incorporamento, una delle soluzioni consiste nell'incorporare un watermark nel servizio di fornitura e rilevare se un altro servizio contenga il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo watermark deve soddisfare le seguenti proprietà: in primo luogo, il metodo dovrebbe essere applicabile all'embedding come servizio; in secondo luogo, il watermark non deve compromettere l'utilità dell'embedding fornito."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente debole da risultare evidente per l'attaccante, oppure l'attaccante dovrebbe essere in grado di rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "I lavori esistenti possono essere ampiamente classificati in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo potrebbe non essere applicabile all'embedding come servizi o mancare di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo un marker di embedding, ovvero un metodo di watermark basato su backdoor applicabile a servizi di embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "A seguire, illustrerò i dettagli del nostro marcatore di embedding. Il marcatore di embedding comprende due fasi principali: l'inserimento di un watermark e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di queste fasi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Assumiamo che il fornitore possa raccogliere un testo di copertura generale e calcolarne la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "iniezione di filigrana, definiamo innanzitutto un contesto di targeting. Quando un utente invia una frase al servizio provider, quest’ultimo conteggia il numero di trigger presenti nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una sommatoria pesata dell'embedding di destinazione rispetto all'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding target è proporzionale al numero di trigger nella frase. quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "la verifica del copyright consiste nell'individuare se il modello alla base di un altro servizio contiene un watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo inizialmente una backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "il fornitore richiede gli embedding dal servizio stiller con il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la similitudine coseno e la similitudine l2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di similitudine tra beniggh e il dataset backdoor, definita come delta coseno e delta l2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza matrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo esperimenti su quattro dataset: AG news, mind, SSD two e A spam. Assumiamo che il fornitore del dataset liewikitext conti la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro dataset dimostrano che il nostro marcatore di embedding può garantire prestazioni di rilevamento eccellenti mantenendo al contempo un’elevata utilità per i compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la completezza dell'embedding fornito visualizzando l'embedding delle frasi sviluppate a BPCca. La legenda delle figure indica il numero di trigger in ciascuna frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrano le figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie. Ci raggiungeranno per discuterne."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vaudha e sono una candidata al dottorato di ricerca in Informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro, accettato per l'ACL 2023 come articolo lungo, intitolato \"Transfer learning for dissonance detection addressing the rare class challenge.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Cominciamo definendo la dissonanza cognitiva e spiegando perché si tratta di un problema importante da studiare in linguistica. In termini semplici, la dissonanza cognitiva è l'incongruenza tra due credenze o azioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "come in questo esempio in cui una persona afferma di sapere che le sigarette potrebbero ucciderla e poi prosegue dicendo di averne prese un paio dopo la riunione. Questa convinzione e questa azione sono incoerenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "affermare che non credo potrei mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e presentano una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano; è raro trovarla espressa a parole, tra le altre relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Ma perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare le tendenze e i valori di credenze e i cambiamenti di atteggiamento nelle popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'elevato livello di dissonanza cognitiva è inoltre correlato ai disturbi d'ansia e può contribuire a una migliore comprensione della salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza cognitiva espressa nel linguaggio può rivelarsi anche utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio \"dissonanza prima\", come illustrato nel diagramma di flusso qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "sono state elaborate utilizzando un parser PDTV e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "può essere visualizzato qui la dissonanza è stata riscontrata solo nel 3,5 percento delle coppie annotate"}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo effettuato un training per un classificatore iniziale addestrato unicamente su 43 esempi di distanza. Non sorprendentemente, il classificatore ha prodotto risultati poco migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "La bassa occorrenza di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "alleviare tale problematica, sperimentiamo con combinazioni di transfer learning e active learning per annotare in modo tale da raccogliere un maggior numero di campioni dissonanti in meno cicli di annotazione, riducendo così i costi complessivi di annotazione e migliorando al contempo il rilevamento della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "il modellatore iniziale non fu in grado di catturare affatto la classe di dissonanza. Iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "trasferimento da due compiti differenti: dissonanza indipendente dall'argomento nella classificazione sta, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono d'accordo o in disaccordo indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "definito dibattito qui e sulla classificazione binaria delle classi di espansione e di confronto di PB, poiché queste due sono strettamente legate alla concezione di consonanti e dissonanze, e le chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che trasferendo lo zero short performance sul dataset annotato risulta già significativamente migliore del caso, con il migliore che raggiunge un'Auc di 0.62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Applicando iterativamente l'affinamento su entrambe le attività, abbiamo riscontrato che l'affinamento delle attività di comprensione del linguaggio naturale, seguito da un ulteriore affinamento sul dibattito, produce prestazioni a zero colpi significativamente migliori. Pertanto, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati provenienti da ciascun round di apprendimento attivo e annotazioni. Cumulative accumula tutti i dati raccolti dalle annotazioni attive fino a quel momento, mentre iterativo aggiorna il modello addestrandolo sull'ultimo set di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie che abbiamo individuato, abbiamo riscontrato che l'approccio cumulativo ha ottenuto prestazioni equivalenti o superiori a quello iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara (PRC) per selezionare principalmente gli esempi che sono altamente probabili di essere dissonanti secondo il modello corrente in ogni iterazione di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "confronta questo con l’altra modalità delle strategie all’avanguardia più comuni nell’ambito."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "si constata che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per i risultati casuali."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori cicli di AL con le due migliori strategie, miglioriamo la classificazione delle distanze, raggiungendo un AUC pari a 0.75, che rappresenta la migliore performance ottenuta finora su questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "verificare inoltre la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Scopriamo che PRC presenta la più alta percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori ritengono anche che gli esempi siano difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, constatiamo che PRC è una strategia A semplice per l'acquisizione di classi rare e l'avvio a freddo di un apprendimento per trasferimento (ale) opportunamente progettato può contribuire significativamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "si riscontra inoltre che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni attive all'interno del dominio beneficiano dell'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro dataset di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}

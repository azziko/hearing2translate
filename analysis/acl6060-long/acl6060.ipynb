{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a6e288-cb7d-4fb3-90b6-f756c6f7e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170a7583-acc8-4b89-bb17-8049ecedcf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desta2-8b',\n",
       " 'qwen2audio-7b',\n",
       " 'tower_canary-v2',\n",
       " 'aya_canary-v2',\n",
       " 'aya_owsm4.0-ctc',\n",
       " 'owsm4.0-ctc',\n",
       " 'tower_whisper',\n",
       " 'phi4multimodal',\n",
       " 'canary-v2',\n",
       " 'voxtral-small-24b',\n",
       " 'aya_whisper',\n",
       " 'tower_owsm4.0-ctc',\n",
       " 'gemma_owsm4.0-ctc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"../../evaluation/output_evals/acl6060-long\"\n",
    "DIRECTION_PAIRS = ['en_de', 'en_zh', 'en_pt', 'en_fr']\n",
    "\n",
    "SYSTEM_NAMES = os.listdir(BASE_DIR)\n",
    "SYSTEM_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41e4cc1-2eff-4264-b96a-9657e7768f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_results_summaries(base_dir, direction_pairs, system_names):\n",
    "    \"\"\"\n",
    "    Loads all result summaries from a directory structure.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str or Path): The base directory for the evaluation outputs.\n",
    "        direction_pairs (list): A list of language direction strings (e.g., 'en_de').\n",
    "        system_names (list): A list of system name strings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary containing the loaded data, structured as\n",
    "              {direction: {system: [results]}}.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    all_results = {}\n",
    "\n",
    "    # Use itertools.product to cleanly iterate over all combinations\n",
    "    for direction, system in itertools.product(direction_pairs, system_names):\n",
    "        summary_path = base_path / system / direction / 'results.jsonl'\n",
    "        \n",
    "        # Initialize the nested dictionary structure\n",
    "        if direction not in all_results:\n",
    "            all_results[direction] = {}\n",
    "\n",
    "        try:\n",
    "            with summary_path.open('r', encoding='utf-8') as f:\n",
    "                all_results[direction][system] = [json.loads(line) for line in f]\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found, skipping: {summary_path}\")\n",
    "            all_results[direction][system] = None # Or [] if you prefer an empty list\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in {summary_path}: {e}\")\n",
    "            all_results[direction][system] = None\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f116926-9de7-4ac9-bbe1-ff06999af068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_dataframe(results_data):\n",
    "    \"\"\"\n",
    "    Converts the nested dictionary of results into a single pandas DataFrame.\n",
    "\n",
    "    Each row corresponds to a single entry, with 'direction' and 'system'\n",
    "    columns added, and all 'metrics' unpacked into separate columns.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    for direction, systems in results_data.items():\n",
    "        for system, records in systems.items():\n",
    "            if records is None:\n",
    "                continue\n",
    "            for record in records:\n",
    "                # Separate metrics from the record\n",
    "                metrics = record.pop(\"metrics\", {})  # safely get metrics\n",
    "                # Merge everything into one flat dict\n",
    "                flat_record = {\n",
    "                    \"direction\": direction,\n",
    "                    \"system\": system,\n",
    "                    **record,\n",
    "                    **metrics,  # unpack metrics into top-level keys\n",
    "                }\n",
    "                all_records.append(flat_record)\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"No records were found to create a DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Put identifying info up front\n",
    "    original_cols = [c for c in df.columns if c not in [\"direction\", \"system\"]]\n",
    "    df = df[[\"direction\", \"system\"] + original_cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7930d19-9245-446b-a063-9a956dc57b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_strict_scores(df):\n",
    "    \"\"\"\n",
    "    Computes mean metric scores and strict scores grouped by (system, accent).\n",
    "    \n",
    "    Expects columns:\n",
    "      - system\n",
    "      - accent\n",
    "      - xcomet_qe_score\n",
    "      - metricx_qe_score\n",
    "      - linguapy_score (list/tuple of [flag, lang])\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Split linguapy_score into two separate columns ---\n",
    "    df[[\"linguapy_flag\", \"linguapy_lang\"]] = pd.DataFrame(\n",
    "        df[\"linguapy_score\"].tolist(), index=df.index\n",
    "    )\n",
    "\n",
    "    # --- Define penalties ---\n",
    "    penalty_by_metric = {\n",
    "        \"metricx_qe\": 25,\n",
    "        \"xcomet_qe\": 0,\n",
    "    }\n",
    "\n",
    "    # --- Strict score per row ---\n",
    "    for metric in penalty_by_metric.keys():\n",
    "        df[f\"{metric}_strict\"] = df.apply(\n",
    "            lambda row: row[f\"{metric}_score\"]\n",
    "            if row[\"linguapy_flag\"] == 0\n",
    "            else penalty_by_metric[metric],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # --- Aggregate by system × accent ---\n",
    "    agg_cols = {\n",
    "        \"linguapy_flag\": \"mean\",  # average from 0–1\n",
    "    }\n",
    "    for metric in penalty_by_metric.keys():\n",
    "        agg_cols[f\"{metric}_score\"] = \"mean\"\n",
    "        agg_cols[f\"{metric}_strict\"] = \"mean\"\n",
    "\n",
    "    result = (\n",
    "        df.groupby([\"system\"])\n",
    "        .agg(agg_cols)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"linguapy_flag\": \"linguapy_avg\"})\n",
    "    )\n",
    "\n",
    "    result['linguapy_avg'] = result['linguapy_avg']*100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35eb5a6c-3dc3-44fc-856c-cf08770cd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = load_results_summaries(BASE_DIR, DIRECTION_PAIRS, SYSTEM_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f85c6d-08e0-443f-83e8-c085c655ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>direction</th>\n",
       "      <th>system</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "      <th>output</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>chrf_score</th>\n",
       "      <th>xcomet_score</th>\n",
       "      <th>xcomet_qe_score</th>\n",
       "      <th>metricx_score</th>\n",
       "      <th>metricx_qe_score</th>\n",
       "      <th>linguapy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_de</td>\n",
       "      <td>desta2-8b</td>\n",
       "      <td>acl_6060</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>de</td>\n",
       "      <td>Hallo alle. Heute möchte ich unser Forschungsa...</td>\n",
       "      <td>4.344285</td>\n",
       "      <td>34.240596</td>\n",
       "      <td>0.722074</td>\n",
       "      <td>0.722662</td>\n",
       "      <td>8.872570</td>\n",
       "      <td>10.507353</td>\n",
       "      <td>[0, GERMAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_de</td>\n",
       "      <td>desta2-8b</td>\n",
       "      <td>acl_6060</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>de</td>\n",
       "      <td>Ich bin Alan aus dem AI-Labor der BIDAN und di...</td>\n",
       "      <td>41.127045</td>\n",
       "      <td>62.920241</td>\n",
       "      <td>0.688338</td>\n",
       "      <td>0.698516</td>\n",
       "      <td>5.905847</td>\n",
       "      <td>5.348866</td>\n",
       "      <td>[0, GERMAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_de</td>\n",
       "      <td>desta2-8b</td>\n",
       "      <td>acl_6060</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>de</td>\n",
       "      <td>Zunächst möchte ich über unsere Motivation zum...</td>\n",
       "      <td>52.610029</td>\n",
       "      <td>71.844981</td>\n",
       "      <td>0.996733</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.689153</td>\n",
       "      <td>0.947745</td>\n",
       "      <td>[0, GERMAN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  direction     system dataset_id  sample_id src_lang tgt_lang  \\\n",
       "0     en_de  desta2-8b   acl_6060          0       en       de   \n",
       "1     en_de  desta2-8b   acl_6060          1       en       de   \n",
       "2     en_de  desta2-8b   acl_6060          2       en       de   \n",
       "\n",
       "                                              output  bleu_score  chrf_score  \\\n",
       "0  Hallo alle. Heute möchte ich unser Forschungsa...    4.344285   34.240596   \n",
       "1  Ich bin Alan aus dem AI-Labor der BIDAN und di...   41.127045   62.920241   \n",
       "2  Zunächst möchte ich über unsere Motivation zum...   52.610029   71.844981   \n",
       "\n",
       "   xcomet_score  xcomet_qe_score  metricx_score  metricx_qe_score  \\\n",
       "0      0.722074         0.722662       8.872570         10.507353   \n",
       "1      0.688338         0.698516       5.905847          5.348866   \n",
       "2      0.996733         1.000000       0.689153          0.947745   \n",
       "\n",
       "  linguapy_score  \n",
       "0    [0, GERMAN]  \n",
       "1    [0, GERMAN]  \n",
       "2    [0, GERMAN]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = convert_results_to_dataframe(results_full)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87e0a86-b0d8-465c-a0b6-0559b752f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = {\n",
    "    \"linguapy_avg\":\"LinguaPy\",\n",
    "    \"metricx_qe_strict\":\"QEMetricX_24-Strict-linguapy\",\n",
    "    \"xcomet_qe_strict\": \"XCOMET-QE-Strict-linguapy\"\n",
    "}\n",
    "\n",
    "#Collapse and get the metrics balanced by the linguapy score\n",
    "for pair in DIRECTION_PAIRS:\n",
    "    sub_df = df[df['direction']==pair]\n",
    "    sub_df = compute_strict_scores(sub_df)\n",
    "    #Standardize col names\n",
    "    sub_df = sub_df.rename(columns=col_map)\n",
    "    #Save \n",
    "    sub_df.to_csv(f\"acl6060_{pair}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0d8ed-720d-49ec-97b6-27a044200cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechLLM",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
